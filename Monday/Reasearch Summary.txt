Executive Summary

Company Overview: monday.com is a leading SaaS “Work OS” platform that serves ~250,000 customers worldwide
justjoin.it
. It provides flexible building blocks (boards, dashboards, etc.) for project management, CRM, software development (monday Dev), and more. Over the past decade, monday.com evolved from a monolithic architecture to a microservices-based, multi-region infrastructure. Initially U.S.-only, they expanded to additional AWS regions (e.g. EU) with a privacy-first multi-regional design – each region hosts its own customers’ data to meet data residency and compliance needs
engineering.monday.com
engineering.monday.com
. This means global enterprises can choose where their data is stored, a competitive advantage for privacy regulations. To achieve this, monday.com built out sophisticated routing: user requests go through Cloudflare and an Envoy-based Ambassador API Gateway in Kubernetes, which uses an authentication service to route traffic to the correct region’s cluster
engineering.monday.com
engineering.monday.com
. Each region’s cluster hosts a suite of ~120+ microservices (as of 2023) that power the platform
engineering.monday.com
. Every microservice at monday.com is owned end-to-end by development teams and runs in containers on Kubernetes (EKS on AWS), typically written in TypeScript/Node.js and some Python, all deployed via Infrastructure-as-Code and GitOps. For example, they manage infrastructure with Terraform and even built an in-house Terraform CI tool called Ensemble for safer, automated changes
engineering.monday.com
engineering.monday.com
. They also built an internal developer platform “Sphera” to let developers self-service common tasks (create new services, provision dev environments, etc.) with best practices baked in
engineering.monday.com
engineering.monday.com
. This engineering culture of investing in internal platforms means monday.com often prefers customized solutions that integrate tightly with their systems (rather than one-size cloud services) when needed.

AI/ML at monday.com: In the last couple of years, monday.com has fully embraced AI as a core part of its product strategy. They have launched features like the monday AI Assistant (“monday AI Sidekick”) and AI Blocks that let users automate workflows with AI (e.g. auto-summarize updates, classify items by sentiment/urgency, translate text, etc.)
monday.com
monday.com
. These features likely use large language models (LLMs) and other NLP models under the hood. Earlier in 2023, monday.com held an “AI Month” initiative where every engineer worked on AI projects
engineering.monday.com
 – a clear sign that the company is infusing AI into both its product and internal development processes. Notably, monday.com’s engineering blog details how they built Morphex, an AI-assisted refactoring system to break up their frontend monolith, completing an 8-year effort in 6 months
engineering.monday.com
engineering.monday.com
. This showcases a forward-looking AI culture: using AI to solve complex internal challenges and speed up engineering work. In terms of product, monday.com’s AI features span multiple domains (workflow automation, project risk analysis, sales intelligence, customer support AI, etc.) and are built to work securely within the Work OS. For instance, they open-sourced an “MCP” (Model Context Protocol) framework
github.com
 to let AI agents safely interact with monday.com data via APIs, with proper authentication and context. All these indicate that monday.com is investing in its own ML infrastructure – fine-tuning models on proprietary work-management data and hosting models to power product features – rather than solely relying on third-party AI APIs. The job description confirms this: the ML Infrastructure (MLOps) Engineer will “build and maintain a platform for model deployment” integrated with monday’s existing stack
justjoin.it
, ensuring models are reliable, scalable, and cost-efficient in production.

Tech Stack & Operations: Monday.com runs on AWS and heavily leverages Kubernetes for orchestration of services (they have K8s experts and even Kubernetes certifications across the team). Continuous delivery is done via CI/CD pipelines and GitOps tooling. For example, they use Codefresh CI and ArgoCD for deployments
engineering.monday.com
, enabling automated canary releases and rollbacks. Observability is a first-class concern: the Observability team (where Jakub works) provides logging, metrics, and tracing infrastructure. Monday uses Datadog and Coralogix for centralized monitoring/logs, and also Prometheus/Grafana for Kubernetes and application metrics
engineering.monday.com
engineering.monday.com
. They instrument services to collect granular telemetry (even customizing Envoy access logs to include user and region tags
engineering.monday.com
) and have dashboards for real-time traffic, latency, errors, etc. High availability and disaster recovery are built in at multiple levels: services are deployed across multiple AZs in each region; if a whole region fails, their architecture allows routing requests to another region’s cluster in read-only mode (though full failover is complex due to the privacy-first data partitioning
engineering.monday.com
). Data is backed up and replicated as needed, and on-call rotations handle incidents 24/7 to meet enterprise SLAs.

Interviewer Profile – Jakub Sokół: Jakub is a DevOps Engineer on the Observability team at monday.com, and his background is strongly cloud-native and Kubernetes-focused. He holds certifications like KCNA, CKA, CKS, and KCSA, indicating deep knowledge of Kubernetes administration, security, and cloud-native tooling. He’s also an AWS Solutions Architect Associate, so he understands AWS services and architecture well. Given this profile, we can expect Jakub to grill on topics like Kubernetes orchestration patterns, cluster scalability, security best practices (network policies, RBAC, container security), and observability frameworks. He likely maintains monday.com’s monitoring systems, so he may ask how to instrument ML services for metrics/tracing or how to integrate with tools like Prometheus, Grafana, and Datadog. Since he’s a security-certified K8s expert, anticipate questions on securing ML deployments (image scanning, least privilege for containers, secrets management, etc.), as well as resilience (pod disruption policies, multi-AZ deployment, backup/restore strategies). Jakub’s observability expertise means he values reliability – expect scenario questions about handling outages, designing alerts for ML model degradation, and ensuring on-call processes are effective. Essentially, the interview will likely probe your practical experience deploying and running ML models on Kubernetes in production: how you handle scaling, CI/CD, monitoring, and troubleshooting – all while keeping costs and security in check. He will appreciate answers that demonstrate both theoretical knowledge and hands-on pragmatism (e.g. knowing how to use Kubernetes features to solve real problems, citing specific tools or configurations you’d use). Tailoring your responses to monday.com’s context (multi-tenant SaaS, global scale, strict uptime and privacy requirements) will show him you’ve done your homework.

Role Requirements Alignment: The ML Infrastructure Engineer role is essentially an MLOps/DevOps hybrid focused on the AI domain at monday.com. Responsibilities include: building the model serving platform (likely on Kubernetes, integrated with monday’s auth and monitoring systems), designing CI/CD pipelines for ML (automated testing, canary deployments, rollback), implementing observability for ML models (latencies, accuracy or drift metrics, etc.), ensuring high reliability (on-call to quickly fix issues), and leading cost optimization efforts for model training and inference (since GPU instances can be costly). You’ll collaborate closely with data scientists/ML engineers – bridging the gap between research and production. In essence, you need to enable rapid experimentation and rigorous production discipline for AI features. This means having knowledge of tools like Docker, Kubernetes controllers (maybe KServe/Seldon for model serving), CI/CD tools, infra-as-code, cloud services for ML (AWS SageMaker, EKS, EC2 with GPUs, S3 for models, etc.), monitoring stacks, and strategies like blue-green or canary deployments. The interviewer will be checking that you can solve real engineering challenges that Monday likely faces: deploying models in a multi-tenant SaaS environment with scalability, security, and observability in mind.

Competitive Landscape: It’s useful to note how peers in the industry approach MLOps, as Monday will want to stay ahead. Many work management and collaboration platforms (Asana, Atlassian/Jira, Notion, ClickUp, Airtable) are adding AI capabilities. Generally, these companies integrate large language models to do things like generate content, summarize updates, predict task deadlines, or provide chatbot assistants. A common pattern is to use third-party LLM providers (e.g. OpenAI, Anthropic) via API, but wrap them with the company’s own data and context. Asana, for example, introduced “Asana AI” which uses OpenAI’s and Anthropic’s LLMs for features like task summaries and smart suggestions
help.asana.com
. Asana learned the hard way that multi-tenant isolation is critical – an incident occurred where an AI agent instance potentially leaked data between customers, underscoring that each user session must be completely isolated in memory/storage
dev.to
. This highlights the need for strong tenancy isolation in any AI infrastructure (e.g. separate containers or contexts per customer or strict permission checks on any data an AI can access). Atlassian (Jira/Confluence) rolled out “Atlassian Intelligence” which uses a hybrid approach: they host open-source LLMs (like Llama 2, Mistral) in-house for privacy, and also use third-party models (OpenAI GPT-4, Anthropic Claude, Google’s models) when appropriate
atlassian.com
. They built a routing system (code-named “Rovo”) to pick the best model per request, and crucially, they offer enterprise customers an option to use only Atlassian-hosted models to keep all data in-house
atlassian.com
. They also enforce that none of the LLM providers retain or train on customer data
atlassian.com
atlassian.com
, similar to Monday’s privacy-first stance. Notion has an AI assistant that integrates GPT-4 and Claude, but like Atlassian, they contractually forbid AI partners from using customer data to train models
notion.com
 and offer encryption and data retention controls
notion.com
notion.com
. Notion also invested in scalable data infrastructure (a huge data lake, vector search, etc.) to support AI features like semantic search and retrieval-augmented generation
zenml.io
zenml.io
. Airtable and ClickUp have added AI to categorize data or generate content within their interfaces, likely leveraging external APIs with similar privacy wrappers. The trend is clear: enterprise SaaS companies are weaving AI deeply into their platforms, but with a heavy emphasis on data security, observability, and cost control. MLOps at these companies means integrating model serving with existing cloud infrastructure, monitoring model quality and usage, and often building custom tooling (if off-the-shelf platforms don’t meet multi-tenant or compliance needs). Monday.com appears to be on the same path: building an AI layer across their Work OS (as evidenced by “AI Blocks,” “AI Assistant,” and their open-source MCP for AI agents). They will expect an ML Infrastructure Engineer to bring best practices from the industry – such as how to do canary deployments of models, how to monitor models for drift or performance regressions, how to optimize GPU utilization – and tailor those to Monday’s use case (multi-tenant SaaS at scale).

In summary, Monday.com is a modern, high-scale cloud product with a rich engineering culture and a rapidly expanding AI footprint. To excel in the interview, you should demonstrate: knowledge of Kubernetes-based system design, especially for ML workloads; an understanding of observability and DevOps practices at scale; familiarity with ML model serving/monitoring techniques; awareness of cost optimization strategies for expensive resources (GPUs, etc.); and clear communication of how you’ve handled real-world incidents and trade-offs. The following sections provide deep-dive preparation: realistic system design scenarios likely to be discussed, an extensive set of technical Q&A (with detailed answers) covering everything from Kubernetes to ML serving to on-call firefighting, coding exercises relevant to MLOps, behavioral question practice (using the STAR method), and intelligent questions you can ask Jakub to demonstrate your enthusiasm and insight. By thoroughly studying these, you’ll be equipped to impress in the interview and show that you’re ready to drive monday.com’s ML infrastructure forward.

System Design Scenarios (MLOps at Scale)

In this section, we present five realistic system design challenges tailored to monday.com’s context. Each scenario outlines a problem the company might face in building out its ML/AI infrastructure, and provides a comprehensive solution approach. The scenarios cover scalability, multi-tenancy, observability, cost optimization, and disaster recovery – all with the constraints of a global, multi-tenant SaaS platform like monday.com in mind. For each, we include trade-off discussions and how the solution aligns with monday.com’s known architecture and the interviewer’s areas of expertise.

Scenario 1: Multi-Tenant, Multi-Region ML Serving Platform

Problem: Monday.com is rolling out a new AI feature – say an “AI Project Assistant” that uses an LLM to answer users’ questions and generate content based on their monday.com data. This service must be available to customers in the US and EU regions and comply with the company’s privacy-first multi-regional architecture. Design a scalable, multi-tenant model serving platform that can deploy this AI assistant across regions. Ensure that each customer’s data stays in the correct region, requests are routed efficiently, and the system can scale to many concurrent users. Consider authentication/authorization (so the model only accesses data the user is allowed to see), low latency requirements, and how to handle tenant isolation so that one customer’s heavy usage or errors don’t spill over to others.

Solution Overview: Build a Kubernetes-based model serving layer in each region, closely integrated with monday.com’s existing service mesh and auth system. There would be separate deployments of the AI assistant service in the US cluster and EU cluster (and any other region Monday supports). Key components:

Inference Service per Region: In each region’s K8s cluster, deploy the AI model as a service (e.g., a Deployment with perhaps multiple replicas of a pod running the model). You might use an industry framework like KServe or Seldon Core to simplify deployment of ML models on K8s, but given monday.com’s tendency to build in-house, they might implement it as a custom microservice (a container that loads the model and exposes a gRPC/HTTP API). Each service instance would load the LLM (or connect to it if using an external API) and handle requests for that region’s users. For scalability, enable horizontal pod autoscaling (HPA) based on CPU/GPU utilization or request throughput – so during peak loads (e.g., Monday morning surge) more replicas spin up, and scale down when idle to save cost.

Global Request Routing: Leverage the existing routing setup (Ambassador Edge Stack with Cloudflare) to direct user requests to the nearest/appropriate region. Monday.com already tags each request at the gateway with the user’s region
engineering.monday.com
. For the AI assistant, you’d have an endpoint like ai.monday.com that the user’s browser or the front-end calls. The gateway’s authentication filter identifies the user and their account’s region (say EU), and then routes the request to the AI service in the EU cluster
engineering.monday.com
. If a request somehow hits the wrong region, Ambassador will proxy it to the correct cluster over their private inter-region link
engineering.monday.com
. This ensures compliance: a EU user’s query gets processed by the EU service (which will only fetch data from databases in EU).

Tenant Isolation Strategies: Multi-tenancy is crucial. Within a region, the AI service is shared by many customer accounts, but we must prevent any cross-talk. Data isolation is handled by Monday’s existing auth: every API call the AI assistant makes to fetch data (e.g., using monday.com’s internal APIs or via the MCP agent framework) should include the user’s auth token and account context, so only their data is retrieved. We’d integrate the model service with monday.com’s SSO/ auth microservice – for example, the service might receive a JWT from the gateway representing the user, and it uses that to call other internal services for context. Additionally, in-memory isolation for the LLM is important if using a persistent session or chain-of-thought: ensure that each session’s conversation history or vector cache is keyed by user/session and segregated. If using an open-source LLM runtime, we might run one model instance per request or use libraries that support multiple contexts. If using a provider like OpenAI, ensure the requests include no data from other users and that returned info is sanitized. We recall Asana’s incident where an agent leaked data due to session reuse across tenants – to avoid that, do not reuse agent instances or tools across different accounts without resetting state
dev.to
. In Kubernetes, consider using namespaces or separate deployments per major tenant if needed (for example, if a few big enterprise clients demand dedicated model instances for guaranteed performance or compliance, you could deploy an isolated service just for them).

Scalability & Performance: The platform should handle potentially thousands of concurrent requests (monday.com has many users; imagine many hitting the AI at once). Use asynchronous processing and batching where possible. For instance, if multiple requests come in at the same millisecond, the service could batch them to the model if the model supports batch inferencing (many transformer models can handle a batch of queries more efficiently than sequential single queries, especially on GPU). Also utilize GPU acceleration for the model: each pod could be scheduled to a GPU node (using Kubernetes tolerations and node selectors for GPU nodes). NVIDIA’s device plugin would provide GPUs to the container. If the model is large, one GPU per pod might be necessary; if smaller, you can host multiple replicas on one multi-GPU node. We should also plan autoscaling at the cluster level – e.g., configure Cluster Autoscaler on EKS to add GPU nodes when there are pending pods. Since GPU instances are expensive, support scale-to-zero: when no traffic, scale down to maybe 0 or 1 minimal replica. Frameworks like KServe can auto-scale down to zero and cold-start on request, which might be acceptable if requests are infrequent (though cold-start will add latency). Alternatively, keep one warm instance per region to handle occasional requests with low latency, and scale out additional pods on demand.

Latency considerations: Use CDN or edge caching where possible for static parts, but for dynamic AI queries, the model inference will dominate latency (which could be, say, 1-2 seconds for an LLM). To keep users’ experience snappy, minimize overhead: host the model service within the cluster close to where the data is (so it doesn’t call across regions). Also, enable HTTP/2 or gRPC streaming if the model supports partial outputs (stream tokens to the client as they are generated for a faster feel). The multi-region setup ensures each user connects to a geographically closer server (EU users to EU region, etc.), reducing network latency. We also might consider caching at the application level: if many users ask similar queries (e.g., “summarize this task update”), cache recent results in an in-memory store (like Redis within region) keyed by content hash – but given the personalized nature of queries, caching opportunities might be limited.

Authentication & Authorization: The model service must enforce the user’s permissions. It should never return data the user isn’t allowed to see. The design would use Monday’s existing auth microservice: e.g., every request to the AI service goes through Ambassador which attaches user identity. The AI service, when it needs data (say the user asks “How many tasks are past due in Project X?”), will call internal APIs (which already check permissions) or possibly query an indexed knowledge base. A good approach is to integrate with Monday’s GraphQL API or internal service calls using the user’s token, so all permission checks happen upstream. If the AI uses a vector database for semantic search (to retrieve relevant docs for context), partition the vector index by account and ensure queries are filtered by account ID. Each step, from data retrieval to response, should be scoped to the tenant. Logging and monitoring should also include the tenant ID, so any unusual activity can be traced per customer. This isolation approach aligns with Monday’s practice of embedding the account slug in subdomains and tokens to separate tenant data.

Observability & Throttling: Build robust monitoring into the platform. Each inference request should log metrics like response time, model load time, token counts, etc. Use Prometheus metrics to track request rates per region, successes/failures, and perhaps custom app metrics like “tokens generated per request” and “prompt length”. Given multi-tenancy, implement per-tenant rate limiting – e.g., avoid one customer overloading the system. Monday could enforce limits (perhaps based on plan or as a safety) such that no single account can use more than X requests per minute of the AI service. This can be done via a gateway policy (Ambassador or a service mesh like Istio could rate-limit by auth token or IP). Also, the model service itself can have an internal queue and apply backpressure if it’s overwhelmed (returning 429 or a friendly “please wait” to clients). Observability also means tracing: incorporate OpenTelemetry to trace an AI request from the gateway through to the model service and any internal API calls it makes, so if a request is slow, you can see where the time went (e.g., waiting on a DB or the model inference). Because this is a user-facing feature, set up alerting on key metrics: e.g., if median or 95th-percentile latency spikes above, say, 3 seconds, or error rate goes above 1%, an on-call engineer (potentially you) is alerted to investigate.

Trade-offs and Alternatives: One decision is whether to use an existing ML serving platform (like Kubeflow/KServe or SageMaker endpoints) vs. an in-house service. KServe integrates well with K8s and can auto-scale models, even down to zero, and manage canary versions, etc. However, it might be overkill or not fit perfectly into Monday’s internal tools. SageMaker hosting could offload some work (managing auto-scaling endpoints), but integrating SageMaker with Monday’s custom auth and VPC could be complex, and it might not meet the multi-region data isolation as cleanly (SageMaker endpoints would be region-specific but are another AWS service to manage, and Monday tends to run most of their compute within their own K8s clusters for control). Given Monday’s culture of building internal platforms (see Sphera, Ensemble), they may lean toward running the model serving within EKS for full control and integration. This also avoids latency of leaving the cluster. We must also consider model choice: open-source vs API. If Monday uses OpenAI API for some features, an alternative design is not hosting the model at all but calling the API from a backend service. That simplifies scaling (OpenAI scales for you) but has drawbacks: data is leaving your environment, costs can grow unpredictably per call, and you rely on a third party’s uptime. Monday.com likely does both: for some AI features they might call external APIs (especially generative text with GPT-4 for highest quality), but for others like simple classifications or fine-tuned smaller models, they likely host internally to save cost and ensure data control. Our design assumes we are hosting at least some models internally (the job description’s mention of “optimizing GPU/CPU usage”
justjoin.it
 strongly implies internal hosting of models on GPUs). The trade-off here is cost vs. control: hosting your own models requires engineering effort and expensive GPU instances, but gives lower per-query cost at scale and keeps data in-house. A hybrid could be to use a smaller open-source model internally for quick responses, and fall back to a larger external model for more complex queries – but that adds system complexity.

This design ensures each region’s ML service is self-sufficient and aligned with Monday’s regional data isolation. If the EU cluster goes down, EU customers lose the AI feature until it’s restored (since we wouldn’t serve their data from the US cluster due to privacy promises). If Monday ever wanted a failover, they’d need customer consent to mirror data in another region. More practically, ensure high availability within each region: deploy multiple replicas across different nodes (and across AZs using topology spread constraints). Use a load balancer service (AWS NLB or ALB ingress) for the AI service so that if one pod dies, traffic goes to others. Also implement circuit breakers and timeouts – if the model is unresponsive or overloaded, the service should time out and return a graceful error (“AI service is temporarily unavailable, please try again”), rather than hanging.

Jakub would likely appreciate the Kubernetes-centric approach (deployments, autoscalers, etc.) and the focus on observability and security. Emphasizing how we protect tenant data (through auth and isolation) addresses a critical concern. This scenario basically shows you can design a secure, scalable ML serving solution that plugs into Monday’s platform – by using their multi-region Kubernetes setup, integrating with their gateway/auth (which provides SSO and tenant-aware routing
engineering.monday.com
), and monitoring everything with their observability stack.

Scenario 2: End-to-End Observability for an ML Feature

Problem: Monday.com has deployed a new machine learning model that categorizes user feedback tickets by sentiment (Positive/Neutral/Negative) using a NLP model. After deployment, they want to ensure this model and its surrounding pipeline are fully observable – meaning, the team can answer: Is the model working correctly? How often is it used? Are predictions accurate over time? Are there any bottlenecks or errors? Design an observability plan for an ML service covering logging, monitoring, and alerting. The model is running as a microservice on Kubernetes. Also, consider model-specific monitoring (data quality, concept drift, etc.) in addition to standard service metrics. Essentially, how would you instrument and monitor an ML model in production at Monday, tying into their existing tools (Prometheus, Grafana, Datadog, etc.), so that both DevOps engineers and Data Science team have visibility?

Solution: Observability for ML systems extends the usual three pillars (metrics, logs, traces) with some ML-specific telemetry. Here’s a design for comprehensive observability:

Metrics (Infrastructure and Application): We will collect all the standard service metrics from the model’s deployment and some custom metrics specific to ML predictions. At the infrastructure level, since it’s on Kubernetes, metrics like CPU usage, memory, GPU utilization (if applicable) and pod restarts are already gathered (for example, Monday collects Envoy and system metrics via Prometheus
engineering.monday.com
, and likely uses Datadog agents for K8s too). We’ll ensure the model pod exports metrics like request rate, request latency, error count (e.g., using Prometheus client library in the model server code or Datadog APM for custom metrics). Key metrics to expose:

Throughput: number of predictions per minute.

Latency: distribution of response times (p50, p95, p99). This can be done by a histogram metric in Prometheus (and shown in Grafana) or via Datadog APM which automatically tracks latency percentiles.

Error rate: count of failed predictions (exceptions in the code, or calls where model couldn’t return a result).

Infrastructure: GPU memory usage (if using GPUs, leverage DCGM or nvidia-smi integration to get how much VRAM is used), GPU compute utilization, CPU and memory of the container – these help detect if the model is saturating resources.

Queue length: if we implement an internal queue or batching, monitor queue size and wait time.

In addition to these, instrument application-level metrics about the predictions:

Prediction distribution: e.g., how many inputs are classified as Positive vs Neutral vs Negative. This can be a Prometheus counter with a label for class, incremented each time. This helps observe if the model is biased or stuck (if suddenly 100% of outputs become “Neutral”, that’s a red flag).

Model confidence (if model provides a score or probability). Track the average confidence or the fraction of predictions above a certain confidence threshold.

Upstream data metrics: perhaps monitor the size of input texts (if extremely long inputs start coming in, which slow down the model).

All these metrics would be scraped by Prometheus in the cluster or pushed to Datadog. We would create Grafana dashboards that plot these over time per region and globally. For example, one panel for latency (with threshold lines), one for throughput, one for error rate, and one bar chart for distribution of outputs. Monday’s team can use these to spot anomalies (e.g., error rate spike or distribution shift).

Logging: The model service should emit detailed logs for each request and important internal events. Each log should include request identifiers (trace ID, user ID or account ID, etc. – ensure no PII like actual content unless needed for debug, and if so, guard it). For example, a log entry might say: INFO: RequestID=abc123 Account=acmeinc InputLength=50 ModelPrediction="Negative" Confidence=0.92 ResponseTime=120ms. These logs would go to Monday’s centralized logging (they use Coralogix and possibly Datadog Logs
engineering.monday.com
). Logging is crucial for debugging and auditing: if a customer says “this ticket was misclassified,” engineers can find the log for that request and see what happened. Also, any errors (exceptions) in the model code should be logged at ERROR with stack trace. We’ll ensure the logging level is configurable (so in normal ops maybe only warnings and errors, but can turn on debug if needed). Logs also help with data drift monitoring: We could log summary stats of inputs (like mean sentiment score from a pre-processing model, or just keep sample logs of input text truncated). Over time, by reviewing logs or feeding them to a monitoring job, the data science team can detect if the nature of inputs is changing (e.g., vocabulary drift or length drift).

Tracing: Enable distributed tracing for the request flow. Monday likely has OpenTelemetry or Datadog APM integrated. The model service can be instrumented to produce a trace span for the inference. Since it might call other internal services (if this sentiment model needs to fetch data or call another API), we include those calls in the trace. This way, if a request is slow, trace will show if it was model computation vs waiting on an upstream dependency. Jakub’s observability team will value a trace that follows a user’s request from the front-end, through the gateway, to the model service, and back. It’s possible to integrate with Monday’s existing tracing setup – e.g., their Ambassador gateway can start a trace (Envoy can generate trace IDs), and propagate it via headers. Ensure our service reads those headers so that all logs/metrics can be correlated by trace ID.

Model Quality Monitoring: This is a unique aspect for ML. We need to monitor how the model’s accuracy or quality is over time. Since this is a sentiment model, we might not get immediate ground truth for each prediction (unless later a user labels it). However, we can employ a few techniques:

Shadow Mode & Human Feedback: During deployment, they might run the model in “shadow” alongside existing logic. If there’s existing labels or rules, compare the model’s output to baseline. If any significant divergence, flag it. Also, incorporate a way for users or support agents to correct the model’s categorization (feedback loop). Those corrections are effectively ground truth that can be logged. We can calculate accuracy on those feedback examples and track it as a metric (like “agreement rate” between model and user feedback).

Statistical Drift: Even without labels, we can track distribution of model inputs and outputs over time. If the distribution of outputs shifts drastically (e.g., last month 10% of tickets were “Negative”, this month 50% are), it could indicate either real-world change or model drift. We’d set up a scheduled job (perhaps a small Spark or pandas job running daily) to analyze recent data and raise an alert if distribution changes beyond a threshold. This might not be fully in scope of DevOps, but as MLOps engineer you’d facilitate it (maybe integrate with an ML monitoring tool or custom script).

Data quality checks: Monitor input data for anomalies. For example, if the model expects text in English, but suddenly 20% of inputs are empty or non-English, that’s an issue. We could add simple checks in code (log a warning if an input is empty or extremely long, etc., and count those).

There are emerging tools (like WhyLabs, Evidently AI) for model monitoring that can integrate with Python to log stats. We might deploy one if needed – but given Monday’s build approach, they might rely on their data scientists and the MLOps engineer to craft custom checks and use existing tools (Grafana, etc.) to display results.

Alerting: With metrics in place, configure alerts for key scenarios:

Latency spike: e.g., alert if p95 latency > 500ms for 5 minutes (indicating potential slowness).

Error rate: alert if >1% of requests are errors (could indicate a bug or external API failing).

Throughput drop: if we expect at least X requests but got none (could indicate the service is down or traffic isn’t reaching it).

No predictions of a certain class: if, say, in the last hour the model hasn’t output “Positive” at all, that could be suspicious. This might be more of an FYI alert to data science if distribution skews.

High resource usage: alert if pod CPU > 90% for sustained period (maybe we need to scale up) or GPU memory nearly full (risk of OOM).

Model drift: if we have a statistic for drift (like KL divergence between recent output distribution and historical), alert if it exceeds threshold (this likely requires more advanced pipeline – maybe offline analysis feeding into an alert).

These alerts can be configured in Datadog (since they have the data and can send notifications) or via Prometheus Alertmanager integrated with whatever on-call system Monday uses. The on-call engineer (maybe you) would get paged for urgent ones (like service down or high error rate), whereas data drift alerts might go to the data science team’s Slack channel for review.

Integration with Monday’s Observability Stack: We’ll leverage what Monday already has. They use Datadog for APM and logging, so instrument the service with Datadog’s Python APM library (if in Python) to automatically get request traces and runtime metrics. Datadog could also do real user monitoring if needed (but for backend not so relevant). They use Grafana/Prometheus for some internal metrics
engineering.monday.com
, so ensure our service exposes Prometheus metrics at an endpoint (/metrics). The Monday Observability team might have common libraries or sidecar exporters we should use (e.g., if they have a standard way to push GPU metrics to Prom). We should conform to their standards (naming conventions for metrics, tagging metrics with service:model-service, region:us-east-1, account:XYZ if feasible). This consistency means all dashboards and alerts fit into their single-pane-of-glass.

Visualization & Analysis: Create a dedicated dashboard for the ML service. This dashboard would be used in daily standups or NOC monitoring. It should show real-time status (e.g., last 15 min of latency, error rate) and also trends (past 7 days of volume, any gradual slowdown or drift). Also, since multiple teams care about this model (DevOps cares about uptime, Data Science cares about accuracy), consider making different views: an Ops dashboard (focusing on system health metrics) and a Data Science dashboard (focusing on model quality metrics). For example, the DS dashboard might chart average sentiment score vs. known customer satisfaction ratings, if they can correlate those.

Incident Response & Debugging: In addition to passive monitoring, have runbooks for what to do if something goes wrong. For instance, if an alert says “error rate > 5%”, the on-call should quickly check logs (maybe via Kibana or Coralogix web UI) for error patterns. Perhaps the model service logs the stack trace and we see it’s a particular input causing a crash (e.g., maybe an emoji in text causing an encoding error). With that info, the team can apply a hotfix or rollback the model if it’s a new version causing it. Another scenario: model drift alert triggers – the team might decide to retrain the model or investigate new user behavior. Essentially, observability should tie into actions: define what constitutes “bad” and ensure alerts prompt humans to investigate or automated rollback if severe (for instance, if we deployed a new model version and error rate jumps, an automation could rollback to previous version immediately, similar to how one would handle a faulty microservice deployment).

Trade-offs/Considerations: Monitoring everything has a cost (performance and monetary). We should be mindful to sample logs and traces if QPS is very high. For example, if the service does 100 req/s, not every single request trace needs to be stored – maybe sample 1 in 10 for detailed tracing, but always collect aggregate metrics. Similarly, logging full input text for every request might be too heavy and raises privacy concerns (better not to log exact user text beyond what’s needed). We might instead log a hashed or length of text rather than content. Also, balancing between Prometheus vs Datadog: Monday uses both, and sometimes that means double instrumentation. In practice, we can use a Prom -> Datadog integration or have Datadog scrape Prom metrics. It’s a design detail but ensure not to bloat the system with redundant metrics.

Another consideration is tooling: there are specialized “ML monitoring” services (like Arize, Monte Carlo, etc.) that could be used. However, given Monday’s engineering maturity, they might prefer using open-source or their own pipelines for this. We should highlight using existing tools (Prometheus/Grafana) creatively for ML needs, which shows cost-consciousness. If the interviewer asks about model monitoring frameworks, we can mention them but also note that integration and data privacy are key (e.g., sending all model inputs to a third-party monitoring SaaS might conflict with privacy, so probably we’d do it internally).

By designing logs/metrics/traces as above, we ensure no blind spots: if the model is slow, we’ll see it; if it’s misbehaving, we have at least indirect signals. This is exactly what an Observability-focused DevOps like Jakub wants to hear – that you plan to treat the ML service with the same rigor (if not more) as any production service. The data scientists on the team would also appreciate the ability to get analytics on the model’s performance from these observability data. This scenario demonstrates your ability to instrument an ML pipeline end-to-end, which is crucial for maintaining ML systems in production (models can fail in more silent and data-dependent ways than traditional software, so this comprehensive approach is needed).

Scenario 3: Cost Optimization for GPU-Intensive Workloads

Problem: Monday.com’s AI team fine-tuned a large language model on monday.com data to power the new “monday AI Assistant”. This model gives great results but is expensive to run. In production, it’s deployed on GPU instances (each node with, say, an NVIDIA A10 or V100 GPU) to meet latency requirements. The CFO is concerned about skyrocketing AWS bills for these GPU instances, especially overnight or on weekends when usage is lower. Design a strategy to optimize costs for this GPU-intensive ML service without significantly impacting performance or reliability. Consider both infrastructure-level optimizations (cloud resource management) and model-level optimizations (efficiency improvements). The goal is to ensure we scale resources with demand, minimize idle GPU time, and possibly leverage cheaper options, all while the service remains responsive for users.

Solution: Cost optimization will involve a combination of auto-scaling, right-sizing, using spot instances/discounts, and optimizing the model execution. Let’s break down the strategies:

Dynamic Autoscaling (Scale-to-Zero and Peak Scaling): The most direct way to save money is to not run GPUs when you don’t need them. We implement aggressive autoscaling for the AI Assistant service. For example, using Kubernetes HPA/VPA in combination: if QPS (queries per second) goes to zero for some time, scale down to 1 or even 0 pods. Technologies like KServe can scale to zero and wake on incoming traffic (with a cold start penalty). If using plain K8s, we could schedule a cron job to scale down at night if predictive, but better is event-driven. We might incorporate KEDA (Kubernetes Event-Driven Autoscaler) if, say, requests come via a queue or if we can use custom metrics (like number of active sessions) to scale. Ensuring scale-to-zero means you pay $0 for compute when no one is using the feature (perhaps at 3AM). The trade-off is cold start delay – we have to accept that the first user in the morning might experience a few extra seconds as a pod spins up and loads the model into memory. We can mitigate cold starts by keeping one small instance alive per region at minimum, or by warming up on a schedule (maybe spin one up at 8am each region’s local time as pre-warm).

Scale down during off-hours: Analyze usage patterns (maybe via logs or metrics). If we find usage drops nightly and on weekends, we could schedule capacity reductions. For example, if normally we run 4 GPU pods during the day but only 1 at night, we can automate that schedule via a cron or use AWS Instance Scheduler for underlying nodes. However, manual scheduling is less responsive than metric-based autoscaling. A combination might be best: set a lower floor at night (min pods = 0 or 1) vs day (min pods = 2) in the HPA config.

Cluster Node Scaling & Rightsizing: Ensure the Kubernetes cluster’s node autoscaler is set up to scale GPU nodes intelligently. AWS EKS can have an Autoscaling Group or Managed Node Group for GPU instances. We configure cluster-autoscaler such that when there are idle GPU nodes (no pods requesting them), it terminates them after a cooldown. Conversely, when new pod needs a GPU and none free, spin up a node. This prevents paying for unused GPUs. Additionally, choose the right instance type for the workload: for instance, if our model can fit in 16GB GPU memory, maybe an instance with one GPU (like g4dn.xlarge) is enough; no need for a p3 with 4 GPUs which might stay partially unused. If multiple smaller models can share a GPU, consider using MIG (Multi-Instance GPU) if on NVIDIA A100s, splitting one physical GPU into slices for better utilization. In K8s, you’d advertise MIG devices as resources to allocate fractionally. But if our use case is one big model per GPU, MIG might not help.

Spot Instances / Spare Capacity: Leverage AWS Spot instances for non-critical or easily restartable workloads. For real-time serving, using spot is tricky if the node can be taken away suddenly. However, we could adopt a hybrid: keep a baseline of on-demand instances for reliability, but allow autoscaler to use spot instances for overflow traffic. If a spot instance goes away, its pod will reschedule on another (there might be a brief disruption). If our service is stateless (aside from model loaded, which can reload), it can tolerate restarts. We’d need to ensure quick reloading of model on a new node if a spot node dies. Another angle: use spot for dev/test or for asynchronous ML tasks (like training or batch inference). The question focuses on production service, but it’s worth noting that any periodic training jobs or vector indexing jobs could definitely use spot instances at cheaper cost. Using spot can cut instance costs by 70%+ in AWS, which is huge savings if manageable.

AWS Savings Plans / Reserved Instances: Since Monday.com likely runs these services continuously (especially if usage grows to require at least some baseline GPUs 24/7), they should consider committing to 1-year or 3-year Savings Plans for the GPU instances to get ~30-50% discounts. A combination strategy: purchase a reserved capacity for the minimum expected load (say we know we need at least 1 GPU per region always on average – reserve those), and use on-demand/spot for spiky additional capacity. This ensures high utilization of reserved instances for maximum ROI.

Model Optimization (software-level): Work with the AI engineers to make the model more efficient so that it can run on cheaper hardware or faster (reducing cost per request). Techniques:

Quantization: Convert the model weights to lower precision (FP16 or even INT8). Many transformer models can run in FP16 with minimal accuracy loss and halved memory usage. INT8 quantization can drastically reduce memory and possibly use CPU more effectively. If we quantize, maybe the model can even run on CPU instances for smaller loads. There are libraries (like NVIDIA’s TensorRT, or Hugging Face bitsandbytes for 8-bit) to do this. Quantization could let one GPU handle more requests concurrently or allow using a cheaper GPU model.

Model Pruning/Distillation: Perhaps the fine-tuned model can be distilled to a smaller architecture that is faster. If we have a large model (say 6B parameters), maybe we can fine-tune a 1B parameter model to approximate it, trading off some accuracy for huge speed/cost gains. Distilled models or even using an smaller base model might yield 4x speed improvement. This requires ML effort but is an avenue.

Batching and Pipeline Parallelism: Ensure the serving code batches multiple requests together when possible (as mentioned earlier). If on GPU, you want to utilize the GPU fully. If single requests are small, the GPU may be underutilized – better to batch 10 at a time if latency allows a few extra milliseconds. Also, if a model is too big for one GPU, splitting layers across two GPUs is possible (pipeline parallelism) – but that often increases cost (two GPUs instead of one). For cost-saving, better to use one GPU fully than two half-utilized.

Dynamic instance scaling vs. concurrency: If using something like TensorFlow Serving or custom code, tune the number of threads or concurrent model runs to maximize throughput per instance, so you need fewer instances.

Caching Results: For certain AI tasks (maybe not as much for a freeform assistant, but for things like “summarize this specific text”), implement caching at the application level. If many users request sentiment on the same text snippet (perhaps unlikely in this scenario, but imagine other cases like translation of a common phrase), you can cache the model output. Even a short-term cache (in-memory or Redis with TTL of a few minutes) can absorb repeated queries. This reduces duplicate work. It’s an easy win if usage patterns have repetition.

Load Shedding and Graceful Degradation: To avoid over-provisioning for rare spikes, design the system to handle overload by degrading rather than over-allocating. For example, if suddenly QPS doubles beyond capacity, instead of immediately adding more GPU nodes (which costs money and takes time), the system could respond with a friendly message “The AI assistant is experiencing heavy load, please retry in a moment” for some requests (HTTP 503 or a graceful fallback). This is a business decision – but if we expect occasional bursts that we don’t want to fully provision for, we can cap concurrency to save cost, at the expense of turning away some requests under extreme conditions. Essentially, decide an SLA target (maybe we allow queueing up to X seconds, then start shedding). This prevents chasing the 100% peak with hardware (which would sit idle most of the time).

Profiling and Cost Monitoring: Continuously monitor cost and usage. We would set up CloudWatch or Datadog monitors on GPU utilization and actual cost metrics. Perhaps create a dashboard showing cost per 1000 predictions. If we detect that off-peak usage has certain pattern, adjust accordingly. For example, if on weekends usage is 10% of weekdays, maybe we manually or automatically scale down infrastructure ahead of weekends. Also use AWS Cost Explorer or a custom script to ensure there are no orphaned expensive resources (like if an old dev/test GPU instance was left running, kill it). Instituting a policy like turning off non-prod GPU resources at night is another tactic.

Trade-offs and Impact on Performance: All these strategies have to be balanced with the user experience and reliability:

Scaling to zero saves money but adds cold start latency. If the model load time is large (could be tens of seconds for a multi-GB model), that might hurt UX. To mitigate, one idea is to use a lighter-weight model when cold, or show a loading animation to the user explaining the AI is “thinking.” Monday might find that acceptable given cost savings. Another mitigation is “pre-warm” scheduling as mentioned (start a pod at typical usage hours).

Using spot instances introduces risk of interruption. We need to tolerate that or ensure the critical capacity is on on-demand. Possibly run a mix: e.g., run 1 on-demand GPU always, allow additional 2 on spot. If a spot dies, users might see slightly longer response until the job restarts on another node. But since the service is stateless and can retry quickly, this may be okay. Test the model startup time: if it’s too slow, maybe not great to have frequent restarts from spot reclaim. We could mitigate by saving model weights on the node’s disk so that if rescheduled on same node type, startup is faster.

Model compression might degrade accuracy. We should quantify the accuracy loss vs cost gain. Perhaps accept a 1-2% drop in accuracy if cost reduces 50%. Alternatively, use the high-accuracy model for premium customers and a cheaper model for free tier (a possible strategy if Monday has tiered offerings).

Batching can increase latency for individual requests (waiting to form a batch) but increases throughput. We can keep batch size small to balance this (like batch of 4 or 8 might only add ~50ms delay but double throughput).

With caching, ensure we invalidate or scope caches properly (each user’s data separate – though for an assistant likely user-specific, so caching may only help same user asking same question twice).

By implementing these, Monday.com could significantly cut costs. For example, one anecdote: an AI company reduced costs by over $1M by moving from always-on large clusters to autoscaling data pipelines
zenml.io
. In Monday’s case, if we assume GPU instances cost, say, $2-$3 per hour each, running even 10 of them 24/7 is ~$72k/month. If autoscaling and off-peak shutdown cuts that by 50%, that’s ~$35k/month saved. Spot instances could further reduce the cost for the dynamic portion by 70%. Using Savings Plans on the base load saves another ~30%. Combined, these could easily halve or more the overall cost without losing much performance.

Jakub will expect such concrete tactics. He might also be interested in whether to use AWS managed services like Auto Scaling Groups scheduled actions vs Kubernetes native. As an MLOps engineer, leaning on Kubernetes to handle scaling keeps it within our control and more granular (plus Monday’s infra is very Kubernetes-centric). Also mention using AWS cost tools and perhaps building internal dashboards for cost per model invocation – this shows a product mindset (cost-per-feature tracking).

Finally, consider cost vs performance trade-off communication: you’d collaborate with the product and AI team to decide acceptable latency increase for cost savings. Perhaps you AB test a quantized model to ensure it’s still okay for customers. The result is a solution where infrastructure scales with load, uses cheaper pricing models, and the ML model itself is tuned for efficiency, aligning with Monday’s requirement to balance cost vs performance
justjoin.it
. This scenario demonstrates that you can make smart engineering decisions to save money (a crucial skill in MLOps, as ML infra can get very expensive, and Monday.com as a business needs to manage cloud costs for margins).

Scenario 4: CI/CD Pipeline for ML Models with Canary Releases and Rollbacks

Problem: The team has a new version of the recommendation model that powers monday.com’s item suggestions. They want to deploy it to production. Past incidents have shown that a bad model deploy (with a bug or worse accuracy) can hurt user experience. Design a CI/CD pipeline for deploying ML model versions that includes automated testing, canary deployments, and quick rollback. The pipeline should integrate with monday.com’s existing tools (GitHub, Codefresh/ArgoCD, etc.) and ensure minimal downtime. In essence, how do we safely and continuously deliver updates to a machine learning model service?

Solution: We’ll set up a robust CI/CD process reminiscent of software deployments, but with ML specifics in mind. Key steps:

1. Version Control & Build: Treat model code and configuration as we do any microservice. The model’s repository (or a monorepo) contains everything needed: model training code (if applicable), inference server code (e.g., a Flask or FastAPI app wrapping the model), and maybe the model weights stored in a binary format. A change (new model version) should be represented as, say, a Git tag or commit. The CI pipeline triggers when a new model is ready – perhaps when data scientists push a model artifact to a model registry or when a Git PR with updated code is merged. Using Monday’s CI tool (they use Codefresh CI), the pipeline will: run automated tests on the model code (unit tests for pre/post-processing, integration tests with a small sample input-output to verify the new model’s responses are sane), then build a Docker image for the model server. The image includes the model weights or downloads them in entrypoint. Tag the image with a version (e.g., recommendation-service:v2.3). Push this to Monday’s Docker registry.

2. Automated Validation Tests: Before deploying widely, run a staging deployment and/or offline tests. For example, deploy the model to a staging environment (Monday likely has a staging K8s cluster or namespace). Populate it with some test data or replay real queries (they might have recorded production queries for testing). Evaluate the new model’s outputs vs the previous version on key metrics (accuracy, or business KPI like click-through-rate if applicable). This could be an automated “A/B test” run internally: the pipeline runs a small batch of queries through both old and new model and compares results. If the new model’s metrics regress beyond a threshold, the pipeline can halt and not proceed to prod deployment – requiring a manual review. This ensures we don’t push obviously worse models. Additionally, run performance tests: measure latency/QPS of the new model under load (maybe using a tool like Locust or k6 in staging). If it’s much slower or more memory heavy, that’s a flag (maybe need to tune it before prod).

3. Canary Deployment: Assuming tests pass, we move to production rollout but in a safe, controlled manner. Monday uses ArgoCD for deployments (GitOps). We can use Argo Rollouts, which is Argo’s tool for canary and blue-green deployments on Kubernetes. The idea: in the production cluster, we don’t immediately replace all instances of the recommendation service with v2.3. Instead, we deploy v2.3 alongside v2.2 (the current). For example, if there are 10 pods normally, we start by launching 1 pod of the new version and keep 9 of the old. We configure Argo Rollouts or the service mesh (Ambassador/Envoy) to send a small percentage of traffic to the new version. Perhaps start with 5% of requests to new model, 95% to old. Monitor for some time (say 30 minutes or a few hours) to see how it behaves.

During this canary, we closely watch metrics: error rates of new vs old, latency differences, and any business metrics (does the new recommender lead to more clicks? though that might take longer to evaluate). Monday’s observability stack can facilitate this by separating metrics by version (we can have the new pods labeled and have dashboards comparing them). If any issue is detected – e.g., new version’s error rate is higher or logs show exceptions – the pipeline or on-call can abort the rollout. Argo Rollouts can be configured with automated analysis: for instance, it can query Prometheus to check “is error_rate_new < 2%?”. If not, it can automatically rollback to the stable version
justjoin.it
 (Argo Rollouts supports metric checks gating each step).

If everything looks good at 5%, then the pipeline (or release engineer) gradually increases traffic to, say, 50%. Another observation period. Then 100%. This progressive delivery limits blast radius of any problem. Users largely see the old model until we’re confident in the new one.

4. Rollback Strategy: Despite testing, things can go wrong (maybe an edge-case that only appears at scale). Our plan includes the ability to rollback quickly. Since we haven’t fully swapped out old version until the very end, a rollback during canary is simply shifting traffic back to 0% new, 100% old – which Argo can do in seconds. If we already moved to 100% new and then discover an issue, we must redeploy the old version. To make this fast, we do not immediately remove the old deployment. A good practice is blue-green style: deploy new as a separate ReplicaSet, then switch service pointer. For a time, keep the old pods running (but not receiving traffic). If new fails, switch back to old pods (already warm and ready). This zero-downtime rollback is valuable. Alternatively, if using canary weight, we can just shift weight back while spinning up more old pods if needed.

The pipeline should include a rollback step if any automated check fails or if a manual QA flags an issue. Also have a one-click rollback in our deployment dashboard (ArgoCD UI allows selecting an older replica set and resuming traffic). The on-call or whoever pushes the deploy should be prepared to execute rollback at any sign of trouble – this might be in the runbook.

5. Continuous Deployment vs. Approval: Decide if this is fully automated or requires human approval at some gates (especially because ML changes might need more oversight). Possibly, after passing automated tests, the pipeline notifies the team and someone (perhaps Jakub or a team lead) approves moving to production canary. This adds a check but ensures awareness. Over time, as confidence grows, it could be more automated.

6. Post-Deployment Monitoring: After full rollout, keep an eye on things especially in the first day. We might use an “extended canary” concept: even at 100% rollout, keep checking those key metrics compared to historical baseline for a while. If we see, say, user engagement dropping or an unusual pattern, we might decide to roll back even after hours of deployment. The pipeline can be integrated with monitoring: e.g., if within 2 hours of deploy an alert triggers (like 5xx errors up), automatically rollback and send alerts. This closes the loop between deployment and ops.

7. Model-specific considerations: Unlike code deployments, ML models might degrade subtly (e.g., worse recommendations quality might not trigger a technical alert). So involve product metrics: if possible, have A/B experiments to evaluate a new model’s impact on user behavior. For example, deploy new model to 10% of users and see if their feature usage or satisfaction is up or down. If down, maybe don’t proceed. This requires coordination with product analytics, but it’s an advanced safeguard for model quality. Given time constraints in an interview, focusing on technical pipeline is fine, but mentioning this shows awareness of ML difference (accuracy/regression testing, not just unit tests).

Integration with Monday’s Tooling: Monday uses GitHub – we can implement GitOps where merging to a production branch of a config repo triggers ArgoCD to deploy new image with canary. They have GitHub integration so maybe we do PR checks (like did tests pass?). They use Codefresh CI, so likely we write a Codefresh pipeline yaml with steps: build image, run tests, push image, update ArgoCD manifest (or call Argo Rollouts API to start canary). ArgoCD handles the actual K8s changes. We utilize Argo Rollouts CRDs (canary strategy with steps 5%->50%->100%). We might also integrate with Slack for notifications (“Deployment v2.3 started… 5% canary… success… promoting to 50%”). This keeps everyone informed.

Zero Downtime & Disaster Plan: Using canary means at no point do we drop service – old handles traffic while new comes up. We also should use readiness probes on pods, so a new pod only gets traffic when it’s actually loaded model and ready (this avoids sending requests to a pod still initializing). That combined with Argo’s weight-based routing means no downtime. If something catastrophic happens (both old and new down), we rely on typical rollback or emergency redeploy of old image – but because we always keep an older working version handy, that risk is low.

Testing: It’s worth highlighting some tests we’d automate:

Unit tests on data preprocessing (e.g., if the model expects certain fields, test code that generates input).

A test that loads the model and does a prediction on a known example, comparing to an expected output (integration sanity test).

Perhaps a canary dataset: a small representative set of inputs and expected outputs (from either ground truth or previous model). After each training, see how new model performs on it (to catch any glaring mistakes).

Performance test as mentioned.

Security test: ensure the container image has no critical vulnerabilities (integrate a scan like Trivy in CI). This is part of DevSecOps – since Jakub is CKS certified, he might appreciate mention that we scan container images and ensure no secrets in them, etc., before deploying.

Rollforward: After a successful deploy, archive the previous version in case of future rollback. Also, the pipeline should tag the model with a version and maybe store the model artifact in S3. Everything is reproducible – so if later we find an issue with v2.3, we can retrain/fix and release v2.4, etc. The CI/CD process itself should be under revision control (likely Monday stores pipeline configs in code too).

Trade-offs: Implementing canary adds complexity and slightly longer deployment times (you’re effectively doing multiple deployments in stages). But it’s worth it for high-impact systems. Not every minor model update might need full canary – but for core features, Monday would likely prefer caution given enterprise customers. Another consideration: stateful models. If the model had any state (this one likely stateless per request), but if it had a cache or learning online, canary is harder because two versions might see different data. Here, recommendation model might adapt to user feedback; running two in parallel could diverge. Usually short canary is fine though, and we might disable online learning during canary.

The solution aligns well with Monday’s practice: They already do code rollouts with canaries and quick rollback (their infra team values reducing incident risk
justjoin.it
). Also, by automating tests and gating deployments, we ensure production-ready models as required
justjoin.it
. Jakub will appreciate mentions of ArgoCD/Rollouts (since Monday uses ArgoCD
engineering.monday.com
), monitoring during deployment, and strong rollback plans (since reliability is paramount). We’re basically showing we know how to bring the discipline of DevOps to ML – continuous integration, continuous delivery, continuous monitoring.

Scenario 5: Disaster Recovery and High Availability for ML Services

Problem: Imagine a worst-case scenario: a critical ML service (say, the authentication/authorization ML anomaly detector or an AI-based security monitoring service) is running in production and the entire AWS region (e.g., us-east-1) experiences an outage, or the Kubernetes cluster hosting it goes down. Since monday.com is a SaaS with uptime commitments, how do we design the ML infrastructure for disaster recovery (DR) and high availability (HA)? Specifically, ensure that important ML functionalities remain available or degrade gracefully during regional outages, and that model data (artifacts, features) are not lost. Consider multi-region deployments, backup strategies, and failover mechanisms, all while respecting data privacy constraints (privacy-first – no cross-region data mixing unless allowed).

Solution: True DR for ML services at Monday.com must align with their overall multi-region architecture. Given their privacy-first design, each region operates mostly independently
engineering.monday.com
. So for HA, we focus on within-region redundancy and for DR, on cross-region strategies for the ML components that can fail over without breaking data compliance.

Within-Region High Availability: Ensure the ML service is deployed with redundancy across multiple availability zones (AZs) in the region. On Kubernetes/EKS, that means:

The Deployment has replicas spread across nodes in different AZs (use topology spread constraints or the default scheduler spreads them if zones are labeled).

Use AWS services like a Multi-AZ ALB/NLB for the service’s endpoint, so if one AZ goes down, traffic still reaches pods in other AZs.

If using a stateful store for the model (e.g., a feature store or vector database), use a multi-AZ setup (like Aurora Multi-AZ for databases, or an ElasticSearch cluster across AZs).

Regularly test AZ outage scenarios (chaos engineering). E.g., simulate node failures in one AZ and confirm pods reschedule in others.

This way, a single data center outage won’t take down the service. The service might see reduced capacity, but should still function at lower throughput. With HPA, if half the pods died, the autoscaler can spin new ones in remaining AZs (assuming capacity).

Backups and Data Durability: For any data that ML services rely on (model weights, training data, feature data):

Model artifacts: Store model binaries in a durable storage like S3 (which is multi-AZ by design). Even if cluster goes down, the model file remains in S3. Better, replicate the S3 bucket to another region (S3 Cross-Region Replication) if we might need to load that model elsewhere for DR. Given privacy-first, you might replicate only non-sensitive model artifacts (the model weights themselves might not contain PII, so replicating them to another region is likely fine).

Feature store / vectors: If the ML service uses a database of embeddings or features (which are derived from user data and thus region-specific), ensure that database has backups. For example, if using DynamoDB or RDS for features, enable automated backups and snapshots. In a total region loss, you have the snapshot you could load in another region if needed (again, careful with privacy – you might only do that if legally allowed or if the customer accepts failover).

Regular backup tests: Periodically test restoring a model service from backup in a sandbox to ensure backups are valid. Maintain infrastructure-as-code (Terraform) such that you could redeploy the whole environment in a new region if needed.

Multi-Region Active-Active or Active-Passive: This is tricky due to data isolation, but some ML services might not need to use customer private data. For instance, an ML service for general grammar checking doesn’t hold private state per region, it could serve all from one region. But if it does use user data, we can’t mix. Assuming a service per region, a failure of one region means that region’s users lose that service until failover. Monday’s multi-region deep dive indicated they chose privacy over global resilience
engineering.monday.com
, meaning they accept that if region is down, they won’t automatically failover users to another region’s data (to not violate data residency). However, they likely aim to restore the region quickly or have continuity for critical features.

Potential approaches:

Cold standby in alternate region: For each region (US, EU), have a cold standby cluster in another region (e.g., us-west-2 for us-east-1, maybe) that is normally off or minimal, but can be spun up if primary fails. This standby might not have any user data until failover. In an extreme scenario, Monday could decide to activate it and direct users there, possibly with limited functionality (maybe not all data available). They’d have to restore from backups the relevant databases onto that region’s infra. This is a DR procedure that might take hours, but ensures eventual recovery. Given strong privacy commitments, this might only be done if downtime would be catastrophic and with customer agreement (e.g., certain customers might opt-in to cross-region backup).

Real-time replication of non-sensitive data: Some AI features might allow replicating just the ML model and not the underlying data. For instance, if a model primarily uses user data from their own region, you can’t replicate that easily. But if the model is mostly static and calls internal APIs for data, you could stand up the model in another region and have it (temporarily) query across regions (through some secure channel) for user data if the user’s home region is down. This is complex and likely not allowed normally. Alternatively, certain features like global search might already aggregate across regions with carefully anonymized indices. For DR, maybe skip automatic cross region failover unless absolutely needed and allowed.

If Monday ever allowed optional data mirroring (e.g., a customer says it’s okay to mirror their data to a secondary region for HA), then an active-passive replication could exist. But by default, they don’t.

Disaster Recovery Plan: Document runbooks for region outage:

Who declares DR? e.g., if AWS us-east-1 is down hard, a call is made that we’re invoking DR.

Steps: (a) Infrastructure team spins up needed core services in backup region using Terraform. (b) Restore databases from latest snapshot (some data loss possible up to last backup). (c) Deploy ML services and point them to restored data. (d) Change DNS or routing so that account.monday.com that normally goes to region A now goes to region B.

This could take significant time (maybe hours), so it’s last resort. But having it planned can save a lot of time under pressure.

After primary region recovers, decide whether to switch back or continue running in DR region and treat original as new backup.

Graceful Degradation: Not all ML features are absolutely critical. In a pinch, it might be better to disable the AI feature than to have it error out or delay core functionality. For instance, if the AI recommendation service is down, the product can hide recommendations and still function normally (just without suggestions). That’s graceful degradation. We can build circuits in the application: if model service not responding, timeout quickly and return no recommendations rather than hanging. Also the UI can catch errors and not bother the user too much (“No recommendations available right now”). This way, an outage of ML doesn’t necessarily require failing over across regions; you can tolerate downtime more like a feature outage rather than total system outage. It’s key to identify which ML services are critical (maybe an auth anomaly detection could be bypassed if down, etc.). Possibly Monday’s critical path is tasks and boards – AI is augmentative, so a safe failure mode is to operate without AI until it’s back. This should be part of design: always have a default path if AI service unavailable (e.g., fall back to a simpler rule-based logic or just do nothing).

Monitoring and Detection: The key to DR is detecting catastrophic failure quickly. Use synthetic monitoring: e.g., have an external monitor ping each region’s ML endpoint. If region is not reachable at all, alert on-call immediately and escalate that it might be a region outage. Tie in with AWS Health APIs. Quick detection triggers faster response (maybe failover or degrade features as above).

Security & Compliance in DR: Ensure that any DR approach is compliant. If copying data to another region even temporarily, make sure it’s encrypted and access-controlled, and possibly purge it after. Keep records of when failover is invoked for auditing. Jakub with CKS might also be interested in K8s security: ensure that backups or failovers do not accidentally expose data (like if you restore an etcd backup, be cautious with secrets, etc.). Use encryption for data at rest and in transit always (so even backup files are encrypted in S3).

Testing DR: Just like fire drills, periodically simulate a region failure. For instance, in a staging environment, shut down all ML pods in region and see if monitoring detects it and if runbook can be followed to recover. Possibly even do a controlled chaos test in prod (if bold, but many do not do full region failover tests in prod frequently due to risk). At least test components individually.

Example: Monday.com’s EU region fails. Our ML service in EU is down. What do EU users see? If we degrade, maybe the AI features show a message “temporarily unavailable for EU customers due to maintenance”. Meanwhile, internal team works to restore EU region. Or if we had pre-decided a failover, perhaps our system routes EU users to US temporarily for just that feature (with a note that data might leave region under emergency). Most likely, they will opt to just let the feature be offline until EU is back, given privacy priority. But core product still works since it doesn’t rely on that ML feature being up (design for loose coupling).

Trade-offs: Achieving full active-active across regions for every service is ideal for uptime but conflicts with privacy/data residency in Monday’s case. They explicitly chose not to replicate customer data globally
engineering.monday.com
engineering.monday.com
. So the trade-off is that they accept potentially higher downtime in a region-wide failure in exchange for compliance. Our design respects that: we focus on making each region resilient (multi-AZ, backups) and plan for worst-case recovery rather than live failover. This means in a rare AWS region outage, Monday might have some downtime for affected accounts. They can mitigate by quick recovery or partial service availability, but there’s no magic if data can’t teleport across region.

Another trade-off: cost vs HA. Running duplicate infrastructure in another region just in case is expensive if rarely used. A compromise is to keep minimal footprint (maybe a small K8s cluster running at low cost, ready to scale up). Or use infrastructure-as-code to create on-demand (slower). Depending on their tolerance, they might not keep warm standby to save cost.

Conclusion: Our DR/HA strategy is multi-layer: (1) Prevent single points of failure with multi-AZ and redundancy in region, (2) Prepare with backups and infrastructure code for cross-region recovery, (3) Gracefully degrade so the system can function without ML if needed, and (4) Practice with monitoring and drills. This ensures that even if something big goes down, Monday.com can either keep running the important parts or restore them in a reasonable time, fulfilling enterprise reliability needs. Jakub will be looking for thinking beyond “just deploy it in two regions” – he’ll want to see awareness of the real constraints (like data residency and cost) and how to still have a plan for resilience. This answer provides that balance.

Technical Questions & Model Answers (Deep Dive)

Now we’ll go through key technical questions likely to arise, grouped by topic. Each question is followed by a detailed answer that demonstrates both conceptual understanding and practical knowledge, often with examples relevant to Monday.com’s context. We also note possible follow-up questions an interviewer like Jakub might ask, to help you prepare further elaboration. The topics covered include:

Kubernetes & Container Orchestration

ML Model Deployment & Serving

Observability & Monitoring

CI/CD for ML Systems

Cost Optimization

Security & Compliance

AWS Infrastructure

Performance Optimization

Incident Response & On-Call

These answers are tailored to the scenario – a senior MLOps Engineer interviewing with a Kubernetes/observability expert at a SaaS company. They blend general best practices with specific insights (e.g., mentioning Monday.com’s tools or needs where applicable).

Kubernetes & Container Orchestration

Q: How would you design the deployment of a machine learning model on Kubernetes?
A: I would containerize the ML model and deploy it as a scalable service in the Kubernetes cluster. Specifically, I’d create a Deployment for the model server (for example, a Flask app or TensorFlow Serving container that loads the trained model). This Deployment would manage replicas of pods, enabling horizontal scaling. I would expose it via a Service (likely ClusterIP with an Ingress or API Gateway like Ambassador for external access) so that other services or external clients can query the model. To ensure the model can handle varying load, I’d configure a Horizontal Pod Autoscaler (HPA) based on metrics like CPU or custom metrics (e.g., requests per second, or GPU utilization if it’s a GPU-based model). For GPU-based models, I’d use node labeling and taints: label certain nodes as gpu=true and schedule the model pods there (with tolerations for a gpu taint), making use of Kubernetes device plugins for NVIDIA GPUs. I’d also set resource requests/limits appropriately – e.g., request one GPU, certain CPU and memory – to ensure the scheduler places pods correctly and doesn’t overcommit GPU memory. In terms of updates, I’d leverage rolling updates (the Deployment will do a rolling update by default) so that new versions of the model can be deployed with zero downtime (spinning up new pods with the updated model while old ones are phased out). Moreover, I’d include readiness and liveness probes on the model pods: the readiness probe might hit a /health endpoint that the model server exposes once it’s loaded the model into memory, ensuring we don’t send traffic to a pod that isn’t ready (model loading can be time-consuming). The liveness probe will periodically check that the model is responsive (if it hangs or crashes, Kubernetes will auto-restart the container). For multi-tenant scenarios, I might consider deploying multiple instances or even separate Deployments if isolation is needed (or use a single deployment but ensure the app enforces tenant isolation in requests). Logging and monitoring are also part of design: I’d include a sidecar or integrate with something like Prometheus for metrics and ensure logs are output to stdout (to be collected by Fluentd/Datadog). Security-wise, run the container with a non-root user, minimal privileges, and possibly an AppArmor profile – applying Kubernetes best practices for hardening. To summarize, the model on Kubernetes would be a self-contained service, scalable via replicas, observable via probes/metrics, and integrated with the cluster’s networking for access.

Follow-up: The interviewer might ask, “How would you handle versioning of models in this setup?” – I’d answer that I could use separate deployments for v1 vs v2 (and route traffic accordingly, maybe via different service URLs or a routing layer for canary testing), or use a shadow deployment for new versions to test them. They might also ask about using specialized frameworks (like Kubeflow or KServe) – I’d mention KServe can automate some of this (e.g., it has InferenceService CRD to handle scaling and canarying of model versions), but in essence it does similar things under the hood. I’d note that I’m comfortable implementing with plain K8s resources or adopting such frameworks if they align with the team’s stack.

Q: What are some Kubernetes design patterns or features you’d use to ensure high availability and scalability for services like an ML API?
A: For high availability (HA), I’d use replication and multi-node spreading. That means running multiple replicas of the pod (via Deployment/ReplicaSet) so that if one instance fails, others continue serving. I’d want those replicas spread across different nodes and ideally across different availability zones (if the cluster spans AZs) – Kubernetes does zone spreading by default if topology labels are set, but we can also use pod anti-affinity to avoid putting all replicas on the same node. Additionally, using a Service with a built-in load-balancing (which Kubernetes does via kube-proxy or iptables) ensures traffic is distributed to healthy pods. For scalability, I’d leverage Horizontal Pod Autoscalers to automatically add or remove pod replicas based on CPU utilization or custom metrics (like QPS, latency). This allows the system to handle spikes in load. Another feature is Cluster Autoscaler at the node level: as demand grows and more pods need scheduling, new nodes are added (and scale down when not needed) – critical in cloud environment to not pay for unused capacity. I’d also apply Rolling Updates for deployments, which ensures that when scaling up or updating, only a portion of pods are replaced at a time, keeping the service available. If the service is stateful or has startup time (like ML models do), I’d configure maxUnavailable=0 and maxSurge>0 in the Deployment update strategy so it adds new pods before removing old ones, maintaining capacity during deploys. Another pattern: use of Ingress or API Gateway to handle a lot of incoming requests efficiently and possibly do things like path-based routing or canary traffic splitting (some ingress controllers or service mesh can do weighted routing for canary releases – e.g., using Istio’s VirtualService or Ambassador mappings). For high availability at the cluster level, if the application is critical, we might consider multiple clusters (e.g., active-active in two regions), but since the question is more about K8s features: I’d mention Resilience features like PodDisruptionBudgets (to ensure not too many pods are killed during maintenance), and priority classes (to make sure important services get scheduled first on constrained resources). Also, request/limit tuning to avoid one service hogging resources which can affect others (proper QoS). In summary: replicating pods (HA), distributing them, autoscaling (HPA, cluster autoscaler), and using deployment strategies and K8s policies for resilience and scaling make sure our ML API stays highly available and can grow with demand.

Follow-up: An interviewer could ask about vertical scaling or optimizing resource usage – I’d note how Kubernetes can use Vertical Pod Autoscaler (though for ML, horizontal is usually preferred due to parallelism). Or they might ask how to handle a sudden spike if autoscaling might be slow – I’d say we can over-provision a bit or use predictive scaling if known peaks (like Monday morning heavy usage pattern). Also might ask about stateful vs stateless – I’d clarify our ML API is stateless (each request independent), which makes HA easier; if stateful (some model training piece), we’d use StatefulSets or persistent volumes with replication.

Q: Jakub might say: “We have ~120 microservices running on Kubernetes. How do you approach managing configurations and secrets for a new ML microservice in this environment?”
A: Managing config and secrets at scale in Kubernetes requires a centralized, secure approach. For configurations, I would utilize ConfigMaps for non-sensitive configuration values (like model hyperparameters, feature flags, etc.) and Secrets for any sensitive data (like credentials, API keys to external services, or maybe a license key for a model library). Rather than hardcoding any config, the ML service would read from env vars or mounted config files provided by these ConfigMaps/Secrets. Since Monday.com likely has standards (maybe using Helm or Kustomize for deployments via ArgoCD), I’d integrate with that – e.g., define the ConfigMap in the service’s Helm chart values and have ArgoCD apply it. For secrets, security is paramount: I would store them in a secure store (perhaps AWS Secrets Manager or Vault) and sync to K8s secrets. Monday might use external secret operator or sealed-secrets; I’d follow their practice. In Kubernetes, I’d ensure RBAC rules so only the ML service’s namespace can access its secrets. I’d also avoid embedding secrets in images or git – they should only live in the secret store. Monday’s internal dev platform (Sphera) likely automates injecting correct config for each environment
engineering.monday.com
, so I’d hook into that. Additionally, with many microservices, consistency helps: use naming conventions (like <service>-config, <service>-secret). Possibly use config management tools: e.g., if Monday uses Helmfile or ArgoCD ApplicationSets, that can template config across environments. For the ML model specifically, one config might be the model version or S3 path to model artifact – that can be in a ConfigMap so ops can update it to point to a new model without changing code. For secrets, e.g., if the model service needs to call an internal API, it might use a short-lived JWT from a service account – in K8s, we could use service account tokens or an external OAuth. If it needs a DB password, that’s a Secret. Another aspect is updating configs: I’d enable dynamic reloading if possible – e.g., mount ConfigMap as a volume and have the app watch it or use a SIGHUP to reload on change, or simply restart pods on config changes (since Deployment will roll out changes if ConfigMap changes). Regarding tooling: since Jakub has a security background, I’d mention using Kubernetes Secret encryption at rest (so secrets in etcd are encrypted, especially important if multi-tenant cluster). Also, use least privilege: the ML service’s service account should only have permission to read its specific secrets, nothing else. In summary: use K8s ConfigMaps/Secrets, tied into Monday’s GitOps, with strong security and organization, to manage all those settings cleanly.

Follow-up: He might ask, “How do you rotate secrets or handle secret updates?” I’d explain that we could use automation to rotate (maybe external secrets operator can sync new versions, or manual process with ArgoCD decrypting new sealed secret). Also mention if a secret changes, we’ll roll pods (e.g., attach secret as env var triggers pod restart through deployment, thus picking up new secret). Another follow-up: “Have you used any secret management tool like HashiCorp Vault with K8s?” – I could mention familiarity, e.g., Vault Agent Injector to auto-inject secrets as volumes, which some companies use to avoid storing long-lived secrets in K8s at all. Since Monday is heavy on security, acknowledging these patterns would be wise.

Q: Can you explain how Kubernetes handles service discovery and load balancing for microservices?
A: Certainly. In Kubernetes, service discovery and load balancing are built-in via the Service resource. When you create a Service (of type ClusterIP for internal or LoadBalancer/NodePort/Ingress for external), Kubernetes will allocate a stable virtual IP (for ClusterIP) and DNS name (like my-service.my-namespace.svc.cluster.local). Any pods or applications within the cluster can discover the service by that DNS name. Kubernetes’ DNS (CoreDNS) automatically resolves that name to the Service’s cluster IP. The Service, in turn, has a list of Endpoints – essentially the IP addresses of all pods that are selected by the service’s selector (e.g., all pods with label app=my-service). When a client pod makes a request to the Service IP or DNS, kube-proxy on the node intercepts it and load-balances it to one of the pod endpoints. By default, it’s a simple round-robin or random selection (iptables or IPVS rules achieve a form of distributed load balancing). This means if we have, say, 5 pods behind a service, traffic to the service IP will be distributed among those 5. For external load balancing, if you define a Service of type LoadBalancer (in cloud environment), Kubernetes will provision a cloud load balancer (like an AWS ELB/ALB). That LB will route traffic to the nodes, and then nodes forward to pods (again via the service endpoints). More commonly for HTTP, you’d use an Ingress Controller (like nginx, Traefik, or Monday’s Ambassador Edge Stack) – which creates a single entry point and routes by host/path to services. The Ingress controller itself uses a Service (type LoadBalancer) to get an external IP, and then does layer7 routing to backend services. But focusing on internal service discovery: It’s automatic via labels and DNS. If one pod of the service dies or a new one comes, Kubernetes updates the Endpoints list immediately, so traffic will not go to dead pods – only live ones get traffic. That’s how it provides basic load balancing and service discovery out of the box. We often don’t even need to worry about it as developers – just use the service name. In Monday.com’s case, they likely also use a service mesh (just guessing, many do at that scale) – if so, a service mesh (like Istio or Linkerd) adds more sophisticated load balancing (client-side load balancing, retries, etc.), but even without it, Kubernetes’ native mechanism is quite solid. For cross-namespace or cross-cluster discovery, you might use ExternalName services or DNS entries. But usually, within a cluster, any service is reachable by service.namespace.svc.cluster.local. To summarize: Kubernetes uses a combination of DNS and proxy rules to map a logical service name to a dynamic set of pod IPs, distributing traffic among them evenly (and can be session-affinity or weighted if configured).

Follow-up: They might ask, “What about load balancing between clusters or multi-region? How would services in different regions communicate?” I’d answer that Kubernetes by itself doesn’t do cross-cluster service discovery; you’d use something like a service mesh with multi-cluster support or external DNS, or simply treat the other region’s endpoint as external. E.g., Monday uses Ambassador which might route to another cluster’s gateway if needed
engineering.monday.com
. So cross-region is often handled at the DNS/ingress level rather than Kubernetes internal.

Q: How do you ensure a Kubernetes pod running a model doesn’t consume all resources and affect other pods?
A: Resource management in Kubernetes is key to isolating pods. I would set appropriate resource requests and limits on the pod’s container for CPU, memory, and GPU. For example, if my model process is known to use ~2 CPU cores on average but could spike to 4, I might request 2 cores and set a limit at 4 cores. The request ensures the scheduler gives the node enough capacity (so the node won’t be oversubscribed in a way that starves this pod), and the limit ensures the kernel CFS will throttle CPU beyond 4 cores usage, preventing it from eating CPU needed by others. Similarly for memory: if the model normally needs 1GiB but worst-case maybe 2GiB, set request=1Gi, limit=2Gi. This way, if it tries to go beyond 2GiB (perhaps a memory leak), the OOM killer will terminate it rather than it pushing the node into swap or OOMing other pods. For GPU, K8s treats GPUs as a schedulable resource (not time-sliced but exclusive by default) – if I request 1 GPU, that pod can’t use more than one, and no other pod can use that GPU concurrently. So that inherently isolates GPU usage. Another method is using cgroups under the hood (K8s implements these limits with cgroups) which isolate CPU and memory for the container. So one pod can’t steal CPU cycles beyond its quota or memory beyond its cap. We also use quality of service (QoS) classes: pods that have requests=limits (Guaranteed QoS) get strongest guarantees; I’d do that for a critical model perhaps. If a model is not as critical, maybe Burstable class is okay but still, giving it a limit prevents unbounded use. Also, monitoring resource usage helps – we set up alerts if a pod is frequently throttling or near limits, so we adjust rather than let it constantly fight for resources. Another angle is namespace resource quotas: if we have multiple teams or many pods, you can cap total resources per namespace to ensure fairness. In Monday’s case, they might allocate budgets to each microservice team. If so, I’d abide by that and not deploy a model that violates quotas. Ultimately, by defining these requests/limits properly, we achieve a level of isolation – the OS will enforce CPU shares, and memory is isolated to avoid interference. If an ML pod did crash from OOM, it would restart and not bring others down (whereas without limits, it might crash the node). I’d also consider using nice scheduling or priority classes if needed – e.g., give production workloads higher priority than debug ones, so if a node is pressure, lower priority pods get evicted first. And as one more measure, if the model is particularly demanding at startup (e.g., loading weights uses a lot of CPU), we might use init containers to do heavy work with separate limits to avoid spikes in main container. All these ensure one workload can’t exhaust node resources to detriment of others.

Follow-up: The interviewer could drill: “What happens if a pod hits its memory limit?” I’d answer that Kubernetes will mark it OOMKilled and restart it (because cgroup will prevent it from using more, causing allocation failure). That’s better than crashing the whole node. Or “How do you detect if CPU is throttled due to limits?” – I’d mention using metrics like container_cpu_cfs_throttled_seconds. Possibly they might ask about using Vertical Pod Autoscaler to adjust limits automatically – I’d say that’s an option in low variability cases, but for ML I typically profile and set appropriate limits, and we can use VPA in recommendation mode to suggest improvements. Another question might be “Have you encountered noisy neighbors issues in K8s and how to solve?” – I’d respond basically with what I said: properly set limits to avoid it, and possibly isolate critical workloads onto dedicated nodes (taints/tolerations or node pools), e.g., run GPU workloads separately from CPU ones so they don’t contend for CPU. Monday with observability focus likely expects careful tuning.

ML Model Deployment & Serving

Q: What is your experience with deploying machine learning models to production?
A: I have significant experience taking models developed by data scientists and deploying them as production services. In past projects, I’ve handled everything from wrapping the model in a web service API, containerizing it, setting up CI/CD pipelines to push it to a cluster, and monitoring it in production. For example, in one project I worked on, data scientists provided a trained scikit-learn model for predicting user churn. I helped by writing a small Flask API that would load the serialized model (pickle) at startup and expose an endpoint /predict that accepts feature inputs and returns the prediction. I then containerized that Flask app with a Dockerfile, ensuring all the right dependencies and even some optimizations (using gunicorn for better concurrency, etc.). I deployed it on Kubernetes with 2 replicas behind a load balancer, and configured autoscaling because at certain times of month we had traffic spikes (end-of-month reporting where many predictions are requested). I also set up logging of each prediction request (with user and model version) to a central system for later analysis. Beyond simple Flask serving, I’ve also used more specialized serving frameworks: e.g., TensorFlow Serving for TF models and TorchServe for PyTorch models. Those can give performance benefits like running on GPUs or doing batch inference. In one case, we had a NLP model that was expensive per request, so we utilized batching – the serving system would accumulate, say, up to 32 requests or 50ms of time, then run them through the model in one go on the GPU. This improved throughput and lowered cost per query. I also have experience with cloud ML platforms: for instance, deploying models on AWS SageMaker endpoints and Azure ML. But I often found more control and cost efficiency by managing it on Kubernetes (especially in a company like monday.com that already has robust K8s infra). Moreover, I always emphasize the things around the model: adding health checks (like a test inference on startup to verify the model is working), adding metrics (like how long inference takes, how often it's called, etc.), and integrating with monitoring/alerting. I’ve dealt with scaling issues too – e.g., a model that was initially single-threaded became a bottleneck, so I had to either increase replicas or use a multi-threaded approach in the serving code. I’ve also performed A/B deployments of models – deploying a new version in shadow mode to compare outputs before switching it live. And importantly, I collaborate closely with data scientists so that the deployed model meets their requirements (for instance, if they need a certain GPU or specific library version, I incorporate that while also ensuring it doesn’t conflict with ops standards). So in summary, I’ve been through the full lifecycle: packaging models as services, using tools/frameworks to serve them efficiently, deploying via CI/CD to cloud infrastructure, and then monitoring and iterating on those deployments. I understand both the ML aspect (ensuring model outputs are correct and timely) and the operational aspect (reliability, scaling, security of the model service).

Follow-up: They might ask “What about model updates – how do you handle updating a model with minimal downtime?” I’d say I use rolling updates on K8s with readiness checks (load new model, only then start serving). Sometimes I deploy new version parallel and route traffic gradually (canary). Also could mention using feature flags to switch model versions. Another follow-up: “How do you ensure the model’s performance (accuracy) stays good in production?” – I’d mention monitoring predictions vs outcomes, setting up data pipelines to gather real outputs and perhaps retraining triggers if accuracy drifts. I might mention an example of detecting when a model needed re-training due to data drift. This shows end-to-end responsibility not just throwing a model over the fence.

Q: What frameworks or tools have you used for model serving (e.g., TensorFlow Serving, KServe, Seldon, etc.), and why?
A: I’ve explored a few different approaches to model serving, depending on the needs of the project. For example:

I have used TensorFlow Serving for deploying TensorFlow models. In one project, we had an image classification CNN trained in TensorFlow. We exported it as a SavedModel and ran TensorFlow Serving Docker containers. TF Serving is high-performance, written in C++, and supports gRPC which allowed us to get better throughput than a Python Flask app would. It also has nice features like model versioning/hot-swapping (you can load a new version of the model file and it will serve it alongside or instead of the old). We chose it there because of performance and because it was a pure TF ecosystem.

I have not used KServe (KFServing) in production yet, but I’ve evaluated it. It’s a Kubernetes-based model serving framework that can manage deployments of models with autoscaling (including scale-to-zero using Knative). It supports popular frameworks (TensorFlow, PyTorch, SKlearn, XGBoost) through pre-built server containers, and you just supply the model URI. It’s attractive because it standardizes how to serve models and handles canarying, A/B tests at the infrastructure level. If I were building a multi-model platform at Monday (with dozens of models), KServe could simplify things. However, it adds complexity (need Knative and Istio under the hood). In smaller environments or where I need more custom logic, I might stick to custom deployments or Seldon.

Seldon Core is another tool I used in a proof-of-concept. It’s somewhat similar, but also allows you to build inference graphs, chaining multiple models or pre/post-processing in a pipeline. I liked that for a case where we had to call two models sequentially (first one extracted features, second did classification). Seldon let me define that as a single deployment unit with separate containers for each step. In practice, Seldon required some learning curve, and we encountered a few bugs with gRPC, but it’s powerful.

For PyTorch models, beyond just writing Flask, I’ve used TorchServe which is an official PyTorch model server. It’s okay – it simplifies packaging models (you create a “MAR” file with model + handlers). But we sometimes found writing our own lightweight server gave more flexibility (especially if custom pre-processing was needed).

Also, I’ll mention using AWS SageMaker endpoints – which is a managed service, you hand it a model and it provisions instances behind an API. I used this for a quick deployment of a scikit model when we didn’t have infra ready. It’s very convenient, auto-scales and all, but it can be costly and you have less control over the environment.

And of course, plain Python web frameworks (Flask/FastAPI/Starlette) often do the job for simpler models or when you want to embed custom logic. I’ve used FastAPI in particular for a recent project because it’s asynchronous and efficient, and it’s easy to add docs (Swagger UI) so others can test the model API.

The choice depends on trade-offs: performance vs. ease-of-use vs. how standardized you want things. In a Kubernetes-heavy org, I lean towards Kubernetes-native solutions like KServe. If I just need to deploy one or two models quickly, a simple container with FastAPI might be faster to implement and easier to customize (plus easier to debug if something goes wrong). For example, at a previous job we tried KFServing for a new model but had trouble with custom auth and logging, so we instead did a custom deployment to have full control.

In all cases, I ensure whatever tool we use integrates with monitoring (I add Prometheus metrics either via sidecar or using tool’s integration – e.g., KFServing can emit some metrics). And I containerize in a way consistent with the platform (for Monday, that likely means a Docker image in their registry, orchestrated by ArgoCD). If Monday.com is not already using one of these frameworks, I’d not introduce heavy complexity unnecessarily; but if they are standardizing on something like KServe, I’m capable of using it. I keep up with these tools’ evolution (for instance, KServe v1 now supports inference graphs like Seldon does). So yes, I have familiarity with these and can pick the right one for the scenario.

Follow-up: They might ask, “Which would you recommend for our case (monday.com)?” I’d reason it out: Monday has many microservices on K8s, likely multiple models to serve (like sentiment, summarization, etc.), so a standardized approach like KServe or Seldon could reduce custom code and allow non-ops folks to deploy models by just providing model URIs. But adopting it has overhead. Alternatively, building a small internal framework might be okay too (they seem to build internal tools if off-the-shelf doesn’t fit exactly
engineering.monday.com
). I’d possibly lean to KServe because it aligns with Kubernetes, can leverage Knative for scale-to-zero (cost saving), and since Jakub is into K8s, he’d appreciate that solution. Another follow-up: “How do these frameworks handle streaming or real-time constraints?” – I’d mention most are request-response, not streaming, though you can approximate streaming by chunking input. If true streaming needed, a custom solution or using gRPC with client streaming might be needed, which is possible but not out-of-box in those frameworks.

Q: How would you implement model versioning and A/B testing in production?
A: Model versioning is crucial to manage updates. First, I would version the model artifact itself (for example, give each trained model a version number or a unique hash, and store it with that identifier, e.g., model-v10.pkl or a Git tag in a model registry). In deployment, there are a couple of patterns:

Canary deployment: If I have a new model version (v11) that I want to test against the current version (v10), I can deploy v11 alongside v10 and split traffic. In Kubernetes, one way is to have two Deployments (or two sets of pods) and use a load balancer that supports weighted routing. Monday.com’s Ambassador gateway or an Istio service mesh can do this – basically route, say, 10% of requests to the new model and 90% to the old. This would allow me to gather comparative performance and possibly accuracy metrics. I’d monitor key metrics for both and if v11 looks good (no errors, similar or better latency, and presumably better prediction quality), I’d gradually increase traffic to it. Ultimately, if v11 is confirmed good, I’d shift 100% traffic to v11 and then decomission v10.

Shadow testing: Another approach is send 100% traffic to v10 (users only see v10’s results), but simultaneously send a copy of each request to v11 (v11’s response is not returned to the user, just logged). This requires the application or the gateway to duplicate calls. This yields data on how v11 would have performed in real conditions without affecting users. Then we can analyze logs: did v11’s outputs differ a lot? If it’s a regression problem, compute error or whatever metric offline. Shadow mode is great for testing accuracy of a new model under real load safely. The cost is double inference compute for that period.

As for versioning in the API, ideally the API interface remains same between versions so clients don’t need to change. But we might include the model version in responses for debugging. Some systems include a header or part of response like X-Model-Version: v11.

We could also implement model version selection at request time: e.g., a query param or user group mapping to model version. That would facilitate A/B testing where Group A users always see new model, Group B see old, and we compare user-level metrics (like engagement or conversion).

In practice, say we use Argo Rollouts, it supports setting canary weights and automatically adjusting them if metrics are satisfied. That could automate part of A/B. If not using such tools, we can manage via Ambassador by having two routes (like /predict goes to a Mapping that splits traffic). Another simpler approach is put the logic in the app: run both models in one service, and flip a fraction of requests to use new model – but that gets messy and resource heavy on one pod.

Important is to have monitoring and logging for each version: tag the logs by model version, maybe separate Prometheus metrics (like prediction_latency{model_version=10} vs 11). This way we can truly compare.

One more aspect: rollbacks. If v11 is bad, versioning allows quick rollback to v10 (since we didn’t remove it until v11 proven). If we did replace it fully and find out later something’s off, having the old model artifact version means we can redeploy that.

In terms of how models are stored, I might use a model registry (some companies use MLflow Model Registry or just an S3 bucket with naming conventions). The CI/CD could automatically fetch the latest “approved” model from there and deploy. For A/B, maybe the service can load two models into memory (if not too heavy) and route internally. But that could double memory usage, so separate deployments is cleaner.

Given Monday.com’s scale, likely a phased rollout (like canary release) is preferred for key models. They might also do AB experiments for product impact (does the new recommendation model lead to more user clicks?). So part of versioning is also connecting to AB testing frameworks – e.g., gradually enabling a feature flag that uses new model for X% of users and measuring results.

To summarize:

Always label and store model with version.

Use parallel deployment (canary or shadow) to test new vs old in production environment.

Route traffic in a controlled way (using K8s ingress or service mesh or load balancer with weights).

Monitor both versions closely (both system metrics and ideally business metrics).

Proceed to full rollout or rollback based on the data.

This approach mitigates risk of a new model and ensures we only fully switch when we’re confident it’s better.

Follow-up: A likely follow-up: “How would you automate the evaluation of the new model’s performance during canary?” I’d answer that we could incorporate an automated metric check – e.g., use Prometheus or logs to compute if error rate or latency is within acceptable range for v11, and maybe if we have labels/ground truth streaming in, compute a quick accuracy metric. Or simply run an offline validation dataset through v11 after deploy to ensure it’s as expected. Another question: “How do you manage model registry in such a pipeline?” – I’d mention possibly using MLflow or even just artifact storage, and linking CI/CD to pull specific version. They could also ask about multiple models in production at once (like personalized models per customer?) – then versioning each model and having a system to route to the right model (maybe via a lookup of customer->model version) would be needed. I’d answer accordingly, maybe mention using the account slug to choose model variant if that ever occurred.

Q: How would you handle real-time streaming data or online feature generation for an ML model (for example, a model that needs features updated in real-time)?
A: This is a great question because many ML systems don’t just rely on static data – they might need continuously updated features (like a user’s last activity time, or aggregate counts over the last hour, etc.). For a model requiring online feature generation, I’d design a feature pipeline that can produce and serve those features with low latency. A typical approach would be:

Use a stream processing framework (like Kafka + Kafka Streams, or Apache Flink, or Spark Streaming) to compute features in real-time from event data. For example, suppose the model needs “number of tasks completed by user in the last 10 minutes”. As events of task completion stream in (perhaps Monday.com has an event bus or Kafka topic of user actions), we could have a stream job that maintains a rolling count per user (windowed aggregation). This job would output updates to a fast data store or cache.

Choose a fast feature store or cache to store these computed features. It could be Redis (if data is relatively small and needs sub-millisecond access), or a specialized feature store solution. Some companies use a key-value store where key = entity ID (user_id, etc.) and value = feature vector (with fields like count_last_10min). This store is kept up-to-date by the streaming job.

When the model service gets a request for a prediction, it would query this feature store for the latest features for that entity. For performance, this should be an in-memory or very fast lookup (Redis can easily do hundreds of thousands ops/sec). Alternatively, if using something like Feast (an open-source feature store), the model could retrieve features via Feast’s online store API.

If the features are extremely time-sensitive, another approach is to incorporate streaming right into the model service via async processes or something – but separating concerns is usually better (feature computation vs inference).

I’d also consider the consistency of features with model expectations. For instance, if the model was trained with certain feature definitions (like a 10-min window count), ensure the online computation matches that exactly. We often maintain code parity or use the same library for feature calc in batch (training) and streaming (serving) to avoid training-serving skew.

Another angle: if truly streaming prediction is needed (like processing a live event feed through a model continuously, rather than request/response), we might embed the model into a streaming job (like run the model on each event in Kafka Streams). But since the question phrased as “model that needs features updated real-time,” I think it’s more about the dynamic data for features.

Also, consider caching and latency: If the model service calls out to get features on each request, that network hop adds latency. To mitigate, one can pre-fetch or cache recent feature values in the model service if the same IDs are queried frequently. For example, if the same user asks multiple times, cache their features for say a few seconds within the service.

We need to ensure concurrency and freshness: e.g., if an event arrives that changes a feature right as a request comes in, maybe slight staleness is okay or not. Typically, these streams are near-real-time (a few seconds delay at most). If true real-time needed, might directly feed events to the model continuously.

Tools like Flink can handle event time windows and state nicely, and can even serve state via queries (Flink has queryable state, though not commonly used).

At Monday.com scale, likely they have Kafka or similar for events. I’d leverage that. Possibly they use something like Redis Streams or AWS Kinesis if not Kafka. I’d design a pipeline accordingly.

Example to illustrate: Suppose Monday.com has a model to predict if a user will miss a deadline, and one feature is “tasks completed in last hour”. We set up an events pipeline: every time a task is completed, an event goes to Kafka (with user_id, timestamp). We have a small Flink job that groups by user_id, counts events per hour window (sliding or tumbling). The Flink job updates a Redis key for that user with the latest count (maybe it stores counts for last few windows for safety). The model service, when making a prediction for a given user and task, does count = redis.get(user_id + ":tasks_last_hour") which returns say 5. It then builds the feature vector and runs the model. That way, the feature is as fresh as Kafka events are (which might be sub-second behind reality).

We also need to monitor these pipelines – ensure the streaming job is healthy (if it lags or fails, our features get stale, affecting model). We’d put alerts on that. If features are missing, model service could fall back to a default or older value (to avoid crashing).

If Monday’s use-case is not latency-critical but still near-real-time, sometimes they run periodic batch jobs e.g. every 5 minutes compute features and push to DB. That’s simpler but adds some delay. Streaming is more continuous and complex, but for true real-time adaptation it’s needed.

In summary: I would decouple feature computation from inference, use a streaming system to keep features updated in an online store, and have the model service fetch current features on each prediction. This ensures the model’s inputs reflect the latest state. It’s more complex than a static model, but definitely doable with modern data pipelines.

Follow-up: They might ask “Have you used any specific feature store products (like Tecton, Feast) to do this?” I would say I’ve explored Feast – it integrates with Kafka and Redis for exactly this purpose, and it provides a nice interface to get features. I could mention that Monday might consider it if they plan to reuse features across models and ensure consistency between batch (training) and real-time. If I have, I’d describe the experience. If not deeply, I’d say I conceptually know how it works and could implement similar with our existing stack. Another follow-up: “What about concept drift detection in streaming data?” – I’d answer that we can monitor the distribution of incoming feature values or model outputs over time. If we see them drift far from training distribution (could compute stats in the stream job or in monitoring), that triggers a retrain perhaps. This shows awareness of maintaining model accuracy as data evolves.

Observability & Monitoring

Q: What metrics would you monitor for a machine learning model service in production, and how would you set up alerting on those?
A: For an ML model service (which is essentially an API serving predictions), I would monitor a combination of system-level metrics and application/ML-specific metrics:

System metrics:

Latency – distribution of response times (e.g., average, 95th percentile). I’d set up metrics like request_duration_seconds (histogram) with labels for endpoint or model version. We want to catch if the model is responding slowly.

Throughput – requests per second, to see usage patterns.

Error rate – how many requests result in errors (5xx responses, or exceptions in the app). Exposing a counter for errors and total requests, and an alert if error rate > some threshold (e.g., 5% for 5 minutes could indicate a problem).

CPU/Memory/GPU usage on the pods – to ensure we’re not hitting resource limits or have memory leaks. We can rely on Kubernetes metrics for these (like cadvisor metrics, or datadog system metrics).

Pod restarts or crashes – if the model process crashes often, that’s a red flag (maybe out-of-memory or other fatal errors).

Queue length or concurrency (if we have an internal request queue or threadpool, monitor how many tasks waiting, if any).

Application metrics:

Prediction count by type: If the model has different output categories (e.g., classification labels), count distribution. For example, in a sentiment model, how many positives vs negatives predicted. If suddenly 100% of predictions become one class, something might be wrong (either data drift or a model issue).

Model confidence: If model provides a confidence score, track average confidence. A dramatic drop could mean model is uncertain (maybe encountering out-of-domain data).

Feature values stats (if easy to log): maybe track the average or distribution of an important input feature over time. Could help catch if input data distribution is shifting (concept drift). This might be more of an offline analysis thing, but you can do it with Prometheus by labeling certain bucketed values.

External calls latency: If the model service calls other services (like feature store or data API), measure those durations too. Maybe the model itself is fast but waiting on DB.

End-to-end business metric (outside the service but important): e.g., for a recommendation model, maybe track click-through rate of recommendations. That’s not immediate in the service, but we’d feed that back in separately. For observability of model quality, if we have ground truth later, we’d want to monitor something like accuracy or error. In real-time that could be done if we get labels later (like in monitoring, how many predictions were right/wrong).

For alerting:

I would set up an alert on high error rate: e.g., more than 5% of requests in 5-minute window are errors (HTTP 500s). That could indicate the model is crashing or returning invalid responses. The alert would be routed to on-call (since it’s production issue).

Alert on latency: if p95 latency >, say, 2 seconds for a sustained period. This may indicate the service is struggling (maybe need scaling or some bug causing slowness). We choose threshold based on SLA or typical behaviour.

No traffic alert: if suddenly requests drop to zero during expected usage (maybe upstream is down or something). For a critical model, that’s also important.

Resource saturation: e.g., CPU usage consistently above 90% or memory above 90% on pods, which could degrade performance or cause OOM. That might be a warning alert to prompt scaling.

Drift detection: This is advanced, but we could set an alert if distribution skews too much. Perhaps if our positive vs negative ratio changes by more than X from baseline, create an alert for data scientists to investigate. Or if model confidence on average drops below some value (could indicate uncertain predictions widely).

Pod restart looping: e.g., if a pod restarts > N times in an hour, fire an alert (something is crashing repeatedly).

If using GPU, maybe alert if GPU memory is at 100% (could prelude an OOM or just inefficiency).

And of course, basic uptime: if the service is not responding to a health check (like if all pods failing readiness or no pod active), alert immediately (this is essentially downtime alert).

To set these up, I’d use whatever monitoring system in place. Monday.com uses Prometheus and Grafana
engineering.monday.com
 plus Datadog. So for system metrics, Datadog APM might already catch latency and errors (for example, an Apdex or error rate monitor). I’d configure Datadog monitors for error rate and latency. For custom metrics like prediction distribution, I’d use Prometheus metrics and possibly set up PromQL alerts via Alertmanager (or if Datadog can scrape those, then Datadog monitors). Grafana might be used for visualizing these but alerting likely done via Alertmanager or Datadog.

I’d also implement a dashboard that the team can see at a glance: e.g., one panel for latency trend, one for error rate, one for request count, one for, say, top output classes percentage, etc. This helps during normal ops and especially during incident debugging.

Additionally, I wouldn’t ignore logs: I’d ensure logs include any errors with stack traces, and maybe log summary of predictions or any anomalies. While not an alert, we could use log-based alerts if something really bad appears (like an exception “Model file not found” etc. could trigger an alert via log monitoring).

In summary, the goal is to have a well-instrumented model service: we see how it’s performing technically and how the predictions distribution looks, so we can catch both engineering issues and potential data issues. Then tie those metrics to robust alerting so that if anything deviates from the norm beyond acceptable thresholds, the right people get notified to investigate quickly.

Follow-up: They might dig, “How would you differentiate between an infrastructure issue vs a model quality issue with monitoring?” I’d say infrastructure issues manifest in errors/latency (which we alert on). Model quality issues might not trigger those, so we rely on distribution metrics or periodic evaluation with labels – which might be more of an offline alert (like “weekly accuracy dropped”). In some cases, user feedback can be used (e.g., if users explicitly rate predictions, track those). Also might ask “What would you do if you got an alert at 2am that latency is high?” – I’d check if it correlates with high load (scale up?), or if some dependency is slow. Use traces to find where time is spent. Possibly roll back recent deploy if that caused it. Since they want on-call scenarios, I’d respond with a methodical approach (check metrics, logs, maybe quickly add capacity or switch off new model if needed, etc.).

Q: How have you implemented logging and tracing for microservices, especially for ML services where you may want to track predictions?
A: I always treat logging and tracing as first-class concerns. For logging: I ensure the service uses structured logging (JSON or key-value logs) so that they can be indexed and searched easily. In a ML service, each request log might include details like a request ID, timestamp, the input (maybe not full input if it’s large or sensitive, but at least some identifiers like user_id or item_id), the output or at least the predicted class, and the model version used. I also log any warnings or anomalies – e.g., if an input is missing some feature and we substitute default, log that as a warning. If the model yields a confidence below a threshold and we want to note it, log that. These logs are shipped to our centralized system (like Elastic, Splunk, or in Monday’s case, they use Coralogix/Datadog Logs). I use log levels appropriately (INFO for normal ops, WARN for recoverable issues, ERROR for serious issues or exceptions). This allows later analysis – say a customer complains about a weird recommendation, we can find that request in logs and see what happened.

For tracing: I propagate a trace/context ID through the call chain. Typically, I’ll integrate with OpenTelemetry or whatever tracing system in place. In code, for example, with Python and FastAPI, I might use Datadog APM or OpenTelemetry SDK to automatically instrument incoming requests. The trace would include spans like “model inference” (the time spent actually running the model) and maybe “feature fetch from Redis” as another span. If the ML service calls another internal API (like to get additional data), that call will be another span. By collecting these, we can see the breakdown of time and also correlate across services (the trace can continue upstream to maybe the frontend request that triggered the model API).

In one specific case, I implemented tracing for an ML pipeline where a user request hit our API gateway -> microservice -> called the ML service -> which then called an external NLP API. We used Jaeger for tracing. I made sure each part (gateway, microservice, ML service) forwarded the trace headers (X-B3-TraceId and others in that system). The ML service had a span for the external NLP call. When an issue occurred (like high latency), looking at Jaeger showed that, say, the external API was slow or maybe it was the model processing. Without tracing, we’d guess; with tracing, it was clear.

For ML specifically, sometimes I need to correlate model predictions with downstream outcomes (did user click?). Logging the prediction with an ID that also goes into an analytics event helps join those later.

Also, if using a service mesh (like Istio), some tracing and logging can be automatic (the envoy sidecar can generate trace spans for network calls). But application-level insight (like model loading time) I add via custom instrumentation.

Another useful thing: capturing unique IDs like user or item IDs in logs as separate fields so that one can filter all logs for a given user’s activity, which might span multiple services. We might also log a shortened version of input (like if a text is too big, maybe log its hash or first 10 words) to help debug certain edge cases (like the model had an error processing a particular text input – logs can help spot that pattern if we store something representative).

Of course, must be mindful of PII – we try not to log sensitive raw data, especially in a multi-tenant environment. For Monday.com, maybe don’t log actual board content, but logging board ID or item ID is fine.

Lastly, I set up logging so that if the model raises an exception, we capture the stack trace in the logs. A common ML service error might be something like “input vector has wrong dimension” – the stack trace helps pinpoint it. Or out-of-memory might not throw Python error but log would show restarts.

Bringing it together: We’d integrate these logs/traces with Monday’s observability tools. They use Datadog – I know Datadog APM can handle distributed tracing and link to logs (e.g., you can click from a trace to related logs). So I’d leverage that: instrument the code with Datadog’s Python library. Then one can see a flame graph of a request’s timeline and also see the logs for that request ID. This greatly speeds up troubleshooting performance issues or functional issues.

Follow-up: They could ask “How do you ensure trace context is propagated when the ML service calls other services or databases?” I’d answer that for HTTP calls, I pass along tracing headers manually or via a library. For databases or caches that don’t inherently propagate context, I still create a span around the call in code. Tools like OpenTelemetry have instrumentation for Redis, etc., which I can use (so a span “Redis GET” appears in trace). If context propagation is a challenge (some older parts might not support it), at least logs with a common request ID can be used as a fallback correlation.

Another follow-up: “We have many microservices, do you log predictions centrally for analysis?” I’d mention possibly shipping certain logs or metrics to a data warehouse for offline model performance analysis. For instance, log predictions and actual outcomes, then later join to compute accuracy, etc. But that might be more the data science side hooking into logs.

They might also ask about “log sampling” if QPS is very high. I’d say yes, if we get huge volume, we might sample debug logs. But for critical decisions or errors, we log everything. It depends on cost and volume.

CI/CD for ML Systems

Q: Can you describe a CI/CD workflow for building, testing, and deploying a machine learning model, and how it differs from a standard software CI/CD?
A: Sure. A CI/CD workflow for an ML model has some extra steps compared to standard software, mainly due to the model training and evaluation. Here’s how I’d outline it:

Continuous Integration (CI) side:

Code and Model Integration: Developers (or data scientists) commit changes – this could be code changes to the model serving code or new model artifacts (weights). We might treat model artifacts similarly to code by storing them in a repository or a model registry. When a new model is ready (maybe via a training pipeline), that event can trigger the CI pipeline as well.

Automated Testing: We run tests just like normal software. This includes unit tests for any functions (data preprocessing, etc.), possibly integration tests where we spin up the model code and ensure it can load a model file and make a prediction. If training code is part of the repository, we test some of that too (maybe using a small sample dataset to ensure the training pipeline works).

Static checks: Linting, style check, and security scans (for example, check if any library vulnerabilities).

Build Docker Image: Build the container that will serve the model. This includes installing the right ML frameworks, copying the model file or downloading it in entrypoint. E.g., a Dockerfile might start from a Python base image, add model_service code, and either include the serialized model (if it’s small or using multi-stage to copy from an artifact store) or have logic to pull it from S3 at startup (in which case the image is same but model version is an env var or config). The CI ensures the image builds successfully.

Package Model (if separate): In some flows, the model artifact is stored in a registry (like MLflow) and the CI pipeline might register the new model version. Or it could tag the model with a version and push to S3. One difference from standard CI is dealing with large binary model files – we often don’t store those in git, so CI has to fetch or be pointed to them. Using DVC or similar can help version large data outside git.

Automated validation: After building, we might run a smoke test of the container – e.g., start the container locally in CI and hit the health endpoint or run a test inference to ensure everything wired correctly. This catches issues like “model file not found” or library compatibility at runtime.

Push artifacts: Push the Docker image to registry. Also push any model artifact to model store if needed. Tag them with version (like git SHA or semantic version).

Continuous Deployment (CD) side:

Deploy to Staging: Use an automated pipeline (could be ArgoCD in Monday’s case, which watches a git repo of k8s manifests, or Jenkins/Codefresh pushing out). We deploy the new model service to a staging environment first. Run integration tests there – maybe run a batch of sample queries through it, or even run it on a replay of a day’s data (if not too time-consuming) to compare outputs vs expected or vs previous model. Data scientists might manually verify some outputs in staging if needed (not everything can be automated, e.g., checking quality of recommendations).

Canary/Gradual Deployment to Prod: Assuming staging is good, promote to production but via a canary. This is a key difference from typical stateless service deployment: we might want to do A/B tests. So our CD process might update the Kubernetes deployment with the new image but only route some traffic to it (as discussed earlier). With Monday’s ArgoCD, this could be done by updating a Rollout object that has canary strategy. The CD pipeline could pause after deploying canary at 10% traffic and wait for some automated checks or manual approval.

Automated Metric Monitoring: During canary, the pipeline or an automated system observes metrics (latency, errors, perhaps a custom metric like conversion rate if measurable quickly). Some setups use automated canary analysis (like Argo Rollouts can integrate with Prom metrics to decide pass/fail). If metrics look good after X minutes or Y requests, then proceed to increase traffic.

Full rollout: The pipeline then increases to 100% new version. Old version pods are scaled down. The new model is now live.

Post-deployment validation: After full deploy, maybe run a scheduled job to compute performance metrics daily. If any degradation is noticed, we can respond (this is more on monitoring side but ties into CI/CD by potentially triggering a rollback or a new improvement cycle).

Differences from standard software CI/CD:

Model validation: We need to validate not just that code works, but that the model’s predictions are acceptable. This could involve comparing against a baseline (like ensure new model’s accuracy is above old model’s or above some threshold on a test dataset). That step might be manual or automated as part of CI (if data and time permit, one can run evaluation on a holdout set in CI pipeline and fail if, say, accuracy drops).

Data dependencies: CI for ML might need access to sample data or a feature store to run tests, which is more complicated than a typical unit test. We might have to mock data inputs or use a subset to keep it quick.

Long build times: Docker images for ML can be large (with frameworks, etc.) and model files can be large, so CI needs caching (use base images with pre-installed libs, etc.) to be efficient.

Multi-step promotion: While many software changes can go straight to prod with enough tests, for models it’s common to do more cautious rollout because correctness isn’t binary – a model could be “working” but 5% less accurate and that’s bad for business. So involvement of domain experts or A/B testing is more prevalent.

Retraining loop: CI/CD might also include a retraining pipeline on fresh data periodically. For example, continuous training (CT) pipelines might produce a new model each week. Integrating that with CI is an MLOps concern (like using Jenkins to orchestrate data extract, train, evaluate, then if good, automatically push new model through CD). This is an extra layer beyond typical software CI.

As a concrete example: I’d possibly use tools like MLflow for model versioning: after training, log model as version X, then have CI retrieve that and run a deployment pipeline. Or use Kubeflow Pipelines for retraining. But Monday likely expects integrating with their existing CI (Codefresh) and CD (ArgoCD).

In essence, an ML CI/CD has to ensure not only that the code is correct but that the model’s behavior is good. It might involve more artifact management (models, data) and sometimes manual approval at deployment (some companies require a data science sign-off if accuracy changes).

I have implemented such workflows: e.g., we had a Jenkins pipeline that retrained a model weekly, ran an evaluation script that compared metrics to last week – if metrics improved or stayed within tolerance, it automatically built a new Docker image for the model server with the new weights and deployed it to a staging. Then we ran a smoke test in staging and if all good, triggered a production deployment (with a config flag to fall back to old model if needed). That helped us rapidly update models while still keeping an eye on quality.

Follow-up: They might ask “How do you manage the retraining pipeline and integration with CD?” – I’d say possibly treat the retraining as separate but connected: once a new model artifact is output, it triggers the CI as if a new version was committed. Tools like GitOps can be used (e.g., push a new config with model hash to a repo that ArgoCD watches).

Another question: “How do you manage model-specific tests? For instance, ensuring new model predictions align with expectations?” – I’d mention having a curated test dataset and expected outputs (or at least expected range/stats) to test against in CI. Could also mention using unit tests for model logic and integration test hitting an actual running model container.

They might also ask about rollbacks if new model underperforms after deploy – I’d explain we can quickly redeploy the previous model version (since we version everything, the previous image is available, and ideally automated detection of drop could trigger a rollback or at least alert the team to do so).

Cost Optimization

Q: Running ML workloads (training or inference) can be expensive. What strategies do you use to optimize costs in an ML infrastructure?
A: Cost optimization is a multi-faceted approach. Some strategies I’ve used:

Right-sizing resources: Ensure we use appropriately sized instances for the job. For example, for inference, if a model can run on CPU with acceptable latency, don’t use a GPU (GPUs are costly). Conversely, if a GPU can handle 10x throughput of CPU for a model, using one GPU server instead of 10 CPU servers could save costs. I always benchmark models to pick the optimal hardware. Also, choose instance types wisely – maybe an EC2 G4 instance (with T4 GPU) is much cheaper and sufficient than a V100 instance if the model is smaller.

Autoscaling and on-demand provisioning: As discussed earlier, use Kubernetes HPA and cluster autoscaler so we run minimal nodes at idle times. For example, at night if traffic is low, scale down the inference pods and maybe even scale-to-zero if no traffic (some systems allow that). Similarly for training, use AWS spot instances or ephemeral clusters that spin up only when needed (like using Kubernetes jobs or EMR/SageMaker jobs that shut down after training).

Spot instances and reserved instances: For non-time-critical or fault-tolerant tasks (like training jobs, batch inference), I use spot instances (they can be 70-80% cheaper). We have to handle interruptions (checkpointing training often so we can resume). For persistent needs, buy reserved instances or savings plans on AWS to get a discount. E.g., if we know we’ll run 2 GPU servers 24/7 for inference, get a 1-year reserved for those.

Optimize the model itself: Use techniques like quantization, pruning, distillation to make the model smaller and faster, which then requires less compute to run. I’ve quantized models from FP32 to FP16 or INT8 which reduces memory and can allow using cheaper hardware or more models per GPU. Distilling a large model into a smaller one can reduce inference cost dramatically, albeit with some accuracy trade-off that we weigh.

Batch processing when possible: For example, instead of making 1000 individual predictions one by one (each with overhead), batch them in groups of, say, 32 if latency allows a slight delay. Batching improves hardware utilization – the GPU can fill its cores better – so cost per inference goes down. We did this in a text processing pipeline where we accumulated short requests for 50ms and processed in one go, it increased throughput per GPU by ~3x, effectively cutting cost by two-thirds for that service.

Infrastructure optimization: e.g., use EKS on Fargate vs EC2, or use serverless if it’s cheaper at small scale. But heavy ML likely stays on dedicated instances. Still, we can turn off dev/test environments when not in use (like dev clusters only up 9-5). Also manage storage costs: if we have huge datasets or model checkpoints, lifecycle old ones to S3 Glacier or delete if not needed. In training workflows, I stage data efficiently to not incur egress costs – e.g., keep training data in the same region as training instances to avoid data transfer charges.

Monitoring usage to find waste: I set up dashboards for GPU utilization, CPU util. If I see a GPU mostly idle (e.g., running at 10% usage), maybe we consolidate workloads or use smaller GPU models or share that GPU with multiple models (MIG on Nvidia A100 can partition a GPU for multiple models if supported). If memory usage of pods is much lower than requested, we can tune requests down and bin-pack more pods per node to use all resources we pay for.

Scheduling and job queueing: For training jobs, perhaps run them sequentially on one powerful machine instead of simultaneously on multiple lesser machines if that reduces total time instances are active. Or use a job scheduler to allocate GPUs to experiments so they don’t sit idle reserved for someone not using them.

Evaluate managed services vs self-managed: Sometimes using AWS Lambda for lightweight inference could be cheaper than a constantly running EC2 (especially if requests are sporadic, Lambda you pay per use). But Lambdas have cold starts and limits, so for ML we often use our own. But for example, an occasional batch job might be cheaper to run as a Glue or Lambda job than keeping a server on.

Caching results: If certain predictions are repeated (like the same input asked often), cache the output so we don’t recompute it each time. This could be at application level or using something like memoization of model results. For instance, if same image is analyzed multiple times, store the result in a cache keyed by image hash.

Costs visibility: Tag all resources, break down costs by environment, model, or team. Then review monthly bills to identify anomalies or heavy hitters. Perhaps we find an old large instance running a forgotten model – shut it down. Or see that dev/test are using GPU instances when they could use CPU to save money if speed not critical.

In Monday.com context, they specifically mention cost vs performance trade-offs
justjoin.it
, so I’d continuously profile models and loads to see where we can trim cost without hurting user experience. Maybe certain less-used features could run on-demand instead of always-on.

Also consider multi-tenancy vs isolation: Running one model serving process per customer vs one shared model for all. Shared is more efficient if the model can handle it because one process (or one set of pods) serves all tenants, achieving higher utilization. However, if each big customer insists on their own model instance, that could lead to many idle instances. I’d argue for sharing with good isolation in software to save cost, unless absolutely necessary to separate.

To illustrate success: In a previous setup, by enabling autoscaling and spot instances for training, we cut our training infrastructure costs by ~50% monthly. And by model quantization and right-sizing, we managed to run two models on one GPU instead of each on separate, saving hardware costs for inference.

All these strategies are balanced with risk: e.g., spot instances might be interrupted – we mitigate that with checkpointing or fallback to on-demand if spot not available (possibly use a mix so service doesn’t go down).

Follow-up: They might ask “How do you specifically optimize GPU utilization?” – I’d mention multi-model on one GPU if memory allows, using Nvidia MPS (multiprocess service) to allow multiple procs to share GPU efficiently, or MIG on Ampere GPUs to carve up GPU. Also pipeline multiple requests concurrently on GPU (most libraries do this automatically if multiple threads).

Another question: “Have you dealt with specific cost issues in cloud?” Maybe egress costs – for example, if Monday uses OpenAI API a lot, that’s cost per call – maybe cheaper to fine-tune an open-source model to reduce API calls. That’s an optimization trade-off (pay OpenAI or invest in running our own). I’d mention that if relevant. Or “Would you use serverless for ML?” – likely not for heavy models due to memory/CPU constraints and latency, but maybe for lightweight tasks or preprocessing. So mostly keep persistent instances for main inference.

They could ask about cost of data storage/pipelines – ensure to delete stale temporary data, compress data, etc. If using S3, enable intelligent tiering for rarely accessed stuff, etc.

Security & Compliance

Q: What are some security considerations when deploying ML models and handling data in an MLOps context, especially in a SaaS like monday.com where customer data is involved?
A: Security is critical, particularly because ML pipelines often handle sensitive data (like user content in Monday’s case) and the infrastructure must be robust against attacks. Key considerations:

Data protection (at rest and in transit): Ensure any data used for training or inference that contains customer info is encrypted at rest (e.g., S3 buckets with server-side encryption, encrypted volumes for databases). Also, always use TLS for data in transit – model service APIs should be behind HTTPS. If the model service calls internal APIs or databases, use TLS there too if possible, or at least within a secure VPC network. For example, Monday likely ensures all internal service traffic is encrypted or within private networks
engineering.monday.com
. In a multi-tenant SaaS, keep data segmented by tenant – e.g., an ML service should require an account ID and only fetch data for that account.

Access control and authentication: The ML service should verify requests (especially if it’s an internal microservice behind auth, the gateway might handle auth tokens). Only authorized services or users should be able to hit the model endpoint with data. Use strong auth (OAuth tokens, mTLS between services, etc.). Also, limit access to ML model outputs if necessary (for instance, if a model might expose some aggregated data, ensure it’s not leaking one tenant’s info to another).

Secrets management: Any credentials (like database passwords, API keys for external ML services, encryption keys) should be stored in secure stores (K8s Secrets, Vault) and not hardcoded or in code repos. Rotate them periodically. Monday’s CKS background suggests using Kubernetes best practices: enable secret encryption in etcd
atlassian.com
, and minimal RBAC (only give ML service’s service account permission to read its secrets).

Container security: The model runs in containers, so we should secure those: use minimal base images (to reduce vulnerability surface), run as non-root user, drop unnecessary Linux capabilities. Also, scan images for vulnerabilities (CI can integrate tools like Trivy or Anchor). Apply Kubernetes Pod Security Policies or Pod Security Standards – e.g., prevent privileged escalation in the container. Ensure the container only has access to needed resources (maybe mount specific volumes if needed, but avoid broad host mounts).

Network policies: In Kubernetes, enforce network policies so the ML service pod can only talk to the services it needs (e.g., feature store, internal APIs) and not others. That limits lateral movement if compromised. It also should not accept traffic except from allowed sources (though if behind an ingress/gateway that might already control it, but internal open ports should be restricted).

Auditing and Logging: Enable audit logs for data access – we should know which service accessed which data. For training jobs, if they run queries on production databases, log that. If a model service queries sensitive info, log those access patterns. Monday might have compliance requirements (SOC2, GDPR) to track data flows. Also, with logs themselves, be cautious not to log sensitive content. Possibly apply log redaction for PII.

Multi-tenancy isolation: If one model service serves multiple tenants, ensure requests are properly isolated. E.g., do not mix data between tenants in memory or output. Consider using separate instances or at least tagging data with tenant IDs throughout. For training on multiple customers’ data, careful with how data is combined or partitioned (maybe only use aggregated or anonymized data for global models to respect privacy).

Adversarial inputs: ML models can be subject to malicious inputs (like someone might try to inject a payload via input data). For instance, a text generation model could be prompted with something that tries to get it to reveal training data or behave badly (like prompt injection attacks). While more on product side, from security perspective, one might sandbox model execution if possible, or put limits (like max input size to avoid DoS, validation of input format to avoid crashing model library, etc.). Also ensure the model doesn’t load any untrusted code. If using something like pickle models – careful, pickle can execute code. Use safe model serialization (joblib or frameworks’ own formats).

Dependency security: ML code often uses many pip libraries (some could be less common). Keep them updated to patch known vulnerabilities. Monitor CVEs in ML frameworks. If using pre-trained models from third parties, treat them like code from third parties – could they have backdoors? Possibly do some evaluation.

GDPR and compliance: Monday serves EU customers with privacy-first architecture
engineering.monday.com
, so for ML that means, ensure no personal data from EU goes to US inadvertently (so training data and model artifacts from EU accounts stay in EU region). Also, implement data retention policies: if a user requests deletion (Right to Erasure), ensure their data is not lingering in training sets or stored model features. It might involve retraining or removing certain data points. Also document what data is used for ML, because customers may ask. Possibly provide an opt-out if needed for using their data in model training (some strict companies do).

On-call security mindset: If the ML service is compromised or misbehaving (maybe through an exploit in a library), have monitoring (like unusual outbound connections or high CPU might indicate compromise). Use Kubernetes security features – e.g., ensure each pod runs with least privileges, maybe use something like Falco to detect suspicious activity in containers.

Penetration testing: Include the ML endpoints in pentests. They might try SQL injection via input or other tricks. Ensure that any data access layer properly sanitizes input. If using dynamic code eval (some people do for model pipelines), avoid that or sandbox it.

Physical security of models: If models are proprietary, consider who has access to them. You wouldn’t want an outsider to download your unique model. Use access controls on model artifact storage (S3 bucket locked down). Perhaps even encrypt models (though if service loads it, it needs key – but at least at rest).

Resilience to DDoS: Like any API, an ML endpoint can be hammered by requests to cause high cost or outage. Use rate limiting at gateway (esp since ML endpoints can be heavy CPU/GPU). Also ensure autoscaling has limits so it doesn’t spin up unlimited nodes and bankrupt – put a cap and have backpressure.

In summary, deploying ML doesn’t exempt from normal app sec best practices – we apply those, and add special care for data privacy and integrity since ML might aggregate data. I keep security in mind with every step: from data pipeline, to model training (ensuring training data is correct and not poisoned), to serving (ensuring isolation and least privilege).

Follow-up: They might ask “Can you talk about model-specific vulnerabilities, like model poisoning or adversarial attacks?” I’d say yes: if someone can influence training data (poison it), they could corrupt the model’s decisions. We mitigate by securing the data pipeline (only allow trusted data, monitor for anomalies in training data). Adversarial examples (like intentionally crafted inputs to fool model) are harder to fully guard against, but awareness helps – e.g., for image models, maybe run some detection for known adversarial patterns or at least don’t give models undue authority (like not auto-approving a transaction solely by model if risk of adversarial input exists). This is more researchy, but show that I know of it.

Another follow-up: “How do you handle compliance requests like deleting a user’s data from models?” I’d answer: maintain traceability of what data was used to train models. If strictly needed, retrain or fine-tune a model to remove that influence. There are techniques (federated learning or training on aggregated stats to avoid storing raw data) to ease this. Or we might refrain from using highly sensitive data in training in first place. Logging where user data goes (and maybe not storing raw text beyond a window) helps.

Since Jakub’s in Observability, he might care about secure logging – ensure logs themselves don’t leak info and are protected (like not everyone can read logs, which might contain user content). I’d mention that we restrict log access to only those who need it (RBAC in logging system, data masking etc.).

AWS Infrastructure

Q: How have you utilized AWS services in deploying or managing ML infrastructure?
A: I’ve leveraged a variety of AWS services for different parts of the ML lifecycle:

Compute: For model training, I’ve used EC2 instances with GPUs (like p2, p3, g4 families), as well as managed services like AWS SageMaker for training jobs. For instance, SageMaker Training jobs are convenient to spin up a cluster, run training with data from S3, then auto-shutdown. I’ve also used AWS Batch for scheduling batch ML tasks (like running a job array of 1000 inference tasks in parallel, using spot instances to save cost). For serving, I often use EKS (Elastic Kubernetes Service) to deploy model containers (which Monday.com does as well). But I’m familiar with AWS Auto Scaling Groups to host custom deployments too. In one case, we used an Application Load Balancer with auto-scaling EC2 behind it to serve a TensorFlow Serving model – pretty straightforward with target tracking scaling.

Storage: S3 is a backbone – storing training datasets, model artifacts, even logs/metrics sometimes. I ensure to use appropriate storage classes (Standard for frequent, Glacier for archival). Also EFS or FSx for cases where multiple training instances needed a shared file system (for distributed training or shared dataset). For feature storage, we used DynamoDB for quick lookups (like precomputed features keyed by user), which is low-latency and scales well. I’ve also utilized ElastiCache (Redis) as mentioned, to cache features or model outputs.

Networking & Security: VPC setups to isolate environments. For example, deploying SageMaker endpoints inside a VPC so they’re not public. Using IAM roles heavily – e.g., an EC2 or EKS node role that can only access specific S3 buckets (like training data bucket) and nothing else. Or an IAM role for an Lambda that does inference allowed only to certain actions. AWS’s identity and access management is crucial to grant least privilege. I also set up CloudWatch Logs to capture logs from instances or Lambdas to debug training runs. And CloudWatch metrics for custom metrics in some cases (though we typically push to Datadog/Prom).

AWS SageMaker ecosystem: I’ve used SageMaker hosting for models. You provide a Docker image or just a model if using built-in algorithm, and it creates a HTTPS endpoint, handles auto-scaling. It’s convenient but a bit pricy and less customizable. Still, for quick POCs or for serving models without our own infra, it’s good. Also experimented with SageMaker Neo (to compile models for faster inference on edge).

CI/CD with AWS: Tools like CodePipeline/CodeBuild for building ML containers – though in Monday’s case, they use Codefresh/GitHub likely. I’ve deployed infrastructure with CloudFormation/Terraform for repeatability – e.g., define an EMR cluster or EKS via Terraform (Monday uses Terraform heavily
engineering.monday.com
). I’m comfortable describing AWS resources as code.

Serverless for light ML tasks: I’ve deployed a simple model as an AWS Lambda function when it was lightweight (like a regression that could compute in under the 15 min and within memory). Combined with API Gateway, we got a quick scalable endpoint with no server to manage. For heavier models, Lambda’s not ideal, but I have used AWS Fargate (serverless containers) to run on-demand ML tasks.

Data pipelines: Using AWS Glue/Spark for data preprocessing at scale feeding into ML training. Also AWS Step Functions to orchestrate ML workflows (like a sequence: preprocess -> train -> evaluate -> conditional deploy). Step Functions is nice for MLOps automation.

Monitoring/Auditing: CloudTrail to audit actions (like who downloaded model artifacts). AWS Config to ensure certain things (like S3 buckets for ML data) are not public and properly encrypted.

Specific ML services: For example, I’ve tried Amazon SageMaker Ground Truth for labeling data, Amazon Personalize for quick recommendation proof-of-concept (though that’s more a managed solution than our own model).

In practice, a scenario: I set up an EKS cluster on AWS with nodes that had GPUs (used EC2 g4dn instances in a node group). Deployed our model service on it with HorizontalPodAutoscaler. Also used AWS Application Load Balancer Ingress Controller to expose the service externally with proper SSL. For training, our data was in S3, we used an EC2 spot fleet with a distributed training job (PyTorch across 4 GPUs on 2 instances) orchestrated by a Python script that used boto3 to request spot instances and run commands. We used CloudWatch Events to schedule retraining every week. After training, the model file was saved to S3 and the CI pipeline (in CodeBuild) would pick it up, build a new Docker image, and update the k8s deployment via kubectl or ArgoCD. All of this was in AWS and took advantage of auto-scaling, spot pricing, etc.

Another example: using S3 + CloudFront to serve model artifacts to edge devices (we did an IoT deployment of models). CloudFront cached the model binary for quick global delivery.

Follow-up: They might ask “How do you ensure AWS cost control when using heavy resources like GPUs?” – I’d refer to earlier cost strategies: using spot instances, scheduling downtime, rightsizing. Possibly mention AWS Compute Savings Plans for steady-state GPU usage, and using AWS Budget alarms to catch cost anomalies early.

They might also ask “How do you handle cross-region considerations?” – I’d mention Monday’s multi-region, so likely separate AWS stacks per region with replication only where allowed. I ensure not to transfer EU data to US for example. Also might mention using AWS Service Catalog or control tower to maintain consistent infra patterns across regions.

Given Jakub’s AWS SA cert, he might appreciate mention of using things like ALB for gRPC (if we did that, ALB now supports HTTP/2/gRPC) or using ECR for container images (definitely used ECR to store Docker images for models).

Finally, maybe mention AWS IAM roles for service accounts (IRSA) in EKS – a way to let pods directly assume IAM roles. This is great for ML pods that need S3 access: you don’t store credentials anywhere, the pod’s service account gets a role to access exactly the S3 path it needs.

Performance Optimization

Q: How do you go about optimizing the performance of an ML model in production (inference latency/throughput)?
A: To optimize inference performance, I look at several layers:

Algorithmic/model optimization: Choose the right model architecture or complexity for the task and latency budget. For example, a smaller neural network or using techniques like distillation can speed up inference. If latency is critical, sometimes we compromise a bit of accuracy for a simpler model that runs faster. I also ensure any unnecessary steps in the model are removed (e.g., if a model outputs a lot of info but we only need some, simplify it).

Compute optimization: Utilize the appropriate hardware – like run on GPU or even specialized inference chips (AWS Inferentia, etc.) if they give advantage. Also, enable vectorization or use BLAS libraries on CPU (like using Intel MKL, oneDNN which frameworks often do).

Parallelism and batching: As mentioned, batch multiple inputs to utilize hardware fully. If requests are single-threaded by default, I might increase the number of threads or use asynchronous processing to keep device busy. E.g., with PyTorch on CPU, set the number of threads to optimal for the instance. On GPU, use multiple streams or allow multiple concurrent requests if the model framework supports it.

Quantization/Lower precision: Use 16-bit floats or even 8-bit integers for inference. Many frameworks (TensorRT for Nvidia, or ONNX Runtime with quantization) can compress models and use faster math instructions. I’ve quantized a transformer model from FP32 to INT8 and saw about 2x speedup with negligible accuracy loss.

Compiled inference: Use inference engines or compilers like TensorRT, ONNX Runtime, or TVM. For example, convert the model to ONNX and let ONNX Runtime optimize it (it can fuse operations, etc.), or if using TensorFlow, use XLA for just-in-time compile. I’ve used TensorRT on an image model and got a nice throughput boost by compiling it specifically for our GPU.

Caching results or intermediate results: If applicable, cache frequently seen inputs or partial computations. For instance, if the model uses an embedding for a user ID, cache those embeddings so if the same user comes again it’s a quick lookup rather than recompute. If model calls an expensive feature generation routine, cache that.

Efficient pre/post-processing: Sometimes the bottleneck isn’t the model but the data prep or post. I profile the pipeline: e.g., reading an image from disk, decoding JPEG might be slower than the model. In such case, I might do things like keep images in a decoded format if possible or parallelize preprocessing. Using optimized libraries (OpenCV vs pure Python, or numpy vectorized ops instead of Python loops) for prepping input can drastically cut latency.

Profile and remove bottlenecks: I use profilers (line profilers in Python, or torchscript profiling) to find where time is spent. If the model is I/O bound, maybe fetch data differently (load into memory, etc.).

Concurrency and load balancing: If single instance can’t handle needed throughput, scale horizontally with more instances, behind a load balancer. Ensure the load is evenly spread. But before scaling out, ensure each instance is fully utilized (which ties back to multi-threading and batching).

Warm-up and CPU affinity: Keep model loaded in memory (don’t unload between requests). Possibly pin threads to specific cores for more consistent performance (sometimes helps in HPC scenarios). Also warm up the model by running a few dummy inferences on startup so any lazy initialization (like cuDNN autotune) is done before real traffic hits.

Memory optimization: Large models can cause memory swapping if not careful. Ensure the model fits in RAM/GPU memory with headroom. If memory is an issue, maybe distribute parts of model on different devices (e.g., model parallelism) – though that can add overhead.

Asynchronous I/O: If the model service is doing disk or network I/O, use async patterns so it can handle other requests while waiting. E.g., an async web framework to overlap network waits.

Use latest libraries/hardware drivers: Upgrading to newer versions of ML libraries can bring speed improvements (like each PyTorch version often optimizes more ops). Similarly, ensure using the latest CUDA/cuDNN optimized for your GPU.

Batching across users if acceptable: Perhaps accumulate requests over a few milliseconds to batch. But careful with latency SLA – can’t batch too long or users notice delay.

I had a specific success: we had a BERT-based text classification taking ~300ms per request on CPU. We optimized by moving it to GPU (T4), which cut to ~50ms, and then we further quantized it to INT8 and got it to ~25-30ms. We also batch up to 8 texts if they arrive together, achieving maybe ~5ms per text on average in batch. So throughput increased hugely.

Also, I remember to always measure after changes to ensure we actually improved and didn’t regress something else. And monitor production latency to catch any performance drift (maybe due to growing input sizes or such).

In addition to code, at the Kubernetes level I might use pod anti-affinity to not collocate two heavy pods on same node causing contention, or use node isolation (if noisy neighbors degrade performance, separate them).

Follow-up: They might ask “Have you used ONNX or any specific tool for model optimization?” – I’d say yes, I have converted models to ONNX and run them with ONNX Runtime which has optimizations and EPs (Execution Providers) for various hardware. Also mention tools like Numba or vectorizing custom code if I had to.

Another follow-up: “How to ensure optimizations don’t alter the model’s correctness?” – I’d answer that I always validate that the optimized model outputs are essentially the same as original (within numeric tolerance) on a test set. For quantization, we check accuracy drop. We maintain a feedback loop: if we over-optimized and lost accuracy, we roll back or adjust.

Incident Response & On-Call

Q: Describe a time you had to troubleshoot a production issue for an ML service. What steps did you take to identify and resolve the problem?
A: In one instance, I was on-call for an ML-based recommendation service that suddenly started timing out and returning errors, affecting the main app’s recommendations. This happened in the afternoon after a new model deployment earlier that day. Here’s how I responded:

Detection: We got alerts from our monitoring system that the p95 latency for the recommendation API spiked above 5 seconds (normal ~500ms) and error rate went above 10%. Users also noticed slow loading of suggestions. As on-call, I first acknowledged the alert and quickly scanned the dashboard – it showed a sharp performance degradation starting around 2 PM.

Immediate triage: I treated it as a major incident since it impacted user experience. My first step was to see if we could mitigate quickly. I knew a new model version (v2) had been deployed a couple hours ago as a canary, then ramped up to full traffic at around 2 PM – which coincided with the issue. Suspecting the new model, I decided to rollback traffic to the previous model (v1) to stabilize things. Using our deployment tool (Argo Rollouts), I set the canary weight back: 0% v2, 100% v1. Within a few minutes, I saw latency start to recover (the old model was lighter).

Investigation: With the immediate user impact mitigated (error rate dropped back down after rollback), I dug into root cause. I pulled up logs for the new model’s pods around the incident time. I saw a lot of warnings and stack traces – specifically an error where an external feature service call was timing out. The new model apparently was calling our feature store for an additional feature that the old model didn’t use. Under load, those calls piled up and caused thread exhaustion. Essentially, the new model introduced a dependency that wasn’t scaling well (the feature store got overwhelmed, leading to cascading latency and then errors).

I confirmed this hypothesis by looking at metrics: the feature store’s response time spiked at that time too. So the new model was the trigger.

Resolution: We kept the model rolled back to v1 for the day. I communicated to the team what happened. The immediate resolution was to fix the feature store or the model’s usage of it. We found that the feature store query used by v2 was not properly indexed, making it slow. That evening, we added an index (a DB tweak) and also added caching in the model service for that feature so it wouldn’t call every time for the same user in short succession. We also adjusted the model service code to fail more gracefully if feature store is slow (use default rather than hang).

The next day, we redeployed model v2 with those improvements, monitoring carefully. It performed within acceptable range and the incident did not repeat.

Post-mortem: I wrote a post-mortem report documenting that the root cause was an un-optimized dependency in the new model and insufficient load testing of that aspect. Action items included: incorporate feature store performance tests in staging, add better alerting on that feature call latency, and when doing canary, ramp more gradually and test at each stage (we realized we rushed from 10% to 100% too quickly).

So in summary, my approach in troubleshooting was:

Quickly rollback to a known good state to mitigate user impact.

Use monitoring and logs to identify the cause (correlating deploy timeline, checking logs for errors, looking at related service metrics).

Fix or work around the cause (in this case code and DB improvements).

Verify the fix in non-prod and then re-deploy.

Add tests/monitors to catch similar issues earlier.

Follow-up: They might ask “How did you coordinate with the team during that incident?” – I’d mention we had an incident Slack channel, I informed product support that we were rolling back, so customer-facing teams could respond accordingly. Also engaged the DB engineer for the index fix quickly. In a serious outage, maybe a brief Zoom war room.

Or “What’s your process for a rollback generally?” – I’d say we keep previous version ready, in Kubernetes just scale old up or redirect traffic. In this case it was straightforward due to canary config. If not, redeploy old image tag from CI pipeline.

They might also ask “What did you learn and implement after that incident?” – I covered some: like more rigorous canary and including dependency performance in testing. Possibly mention that now I also always include a fallback to old model in code so we can flip quickly if needed.

Q: If an ML model is not performing well in production (e.g., accuracy or output quality has degraded), how would you detect it and respond?
A: Detecting model performance degradation is trickier than catching system errors, but there are ways:

Monitoring business metrics and feedback: I’d rely on proxy metrics or direct feedback. For instance, if it’s a recommendation model, a drop in user engagement (click-through rate on recommendations) could indicate the model’s quality degraded. So we track that over time. If we see a statistically significant dip after a new model version or gradually over time, that’s a red flag. Another example: if we have any human-in-the-loop or user feedback (like users marking recommendations as not relevant, or support tickets complaining), that is valuable input.

Validation with ground truth: For some models, we might get actual outcomes later. E.g., a churn prediction model – we can later see if those users churned. We could set up an automated job to compute the model’s precision/recall on recent data once ground truth is available. If that metric falls below a threshold, alert the team. This might be a slower detection (days or weeks lag), but important for long-term drift.

Drift detection: We can monitor input data distribution. If input data characteristics deviate greatly from training data (e.g., feature averages shifted), it may indicate the model might not perform as well (concept drift). There are statistical tests or tools (like KS tests on feature distributions) that can run periodically. If drift exceeds a threshold, it’s a signal to retrain or check model.

Shadow evaluation: If we haven’t fully replaced an old model, we might shadow-run it and compare outputs. If the new model disagrees often with old and the old was known good, we might double-check if new is indeed better or introduced errors.

Once we detect an issue (say CTR dropped by 15%), I’d dive deeper. Perhaps segment by user type or region – maybe the model is failing for a subset (e.g., it’s not handling a new type of task). Check logs or sample predictions and see if they look obviously off. In one case, we saw an NLP model’s output quality degrade because language usage changed (slang or new terms not in training).

Responding:

If it’s severe and clear that the model is the cause, consider rolling back to a previous model version that had better performance (assuming we still have it and it’s still relevant). For example, if a new model update caused accuracy drop, revert to the older model while investigating.

If it’s gradual (concept drift), plan to retrain the model with fresh data. Maybe schedule an interim retraining or patch the model (sometimes you can fine-tune on new examples to give it a boost on new patterns).

Analyze if additional features or adjustments can fix the performance. Perhaps the model is missing a new signal that emerged; we could incorporate that in next version.

If real users are affected, possibly adjust system behavior in short term. E.g., if spam detection model is letting spam through, temporarily tighten rules (fallback to heuristic filters) until model is fixed.

Communicate with stakeholders – e.g., if this impacts customers, coordinate with support on messaging if needed (“We’re aware recommendations aren’t as good right now and are working on it,” etc., though often such specifics aren’t communicated externally unless it’s critical).

As an example, we had a fraud detection model that started missing fraud cases because fraud patterns changed. We detected this because fraud incidence (a business metric) rose sharply. Response was to retrain the model on the latest data and also quickly deploy a few heuristic rules to catch obvious new fraud until the model was updated. Also wrote a script to regularly retrain every week to keep it up-to-date going forward.

Also consider implementing continuous training or periodic evaluation as part of MLOps pipeline if not already – to catch issues before they become big.

It’s important to differentiate between a model performance issue vs a data issue vs a bug. If a model’s accuracy drop is due to some input pipeline bug (maybe features not being passed correctly), that’s a fix to the pipeline (and you’d likely see weird log errors or distribution shifts). If it’s truly the model, retraining or redesign is needed.

In summary: I’d use monitoring and periodic evaluations to detect a problem, confirm the scope and cause, then either roll back, retrain, or supplement the model as appropriate, with urgency depending on impact. And after resolving, improve our processes – maybe add that metric to alerts so we know sooner next time, or shorten the retraining cycle.

Follow-up: They might ask “What if you can’t get ground truth quickly? How to ensure model stays good?” – I’d say that’s often the case; we then rely on proxy metrics or user feedback. In absence of labels, maybe do periodic manual checks – e.g., sample 100 predictions each month and have an analyst review them. Or maintain a holdout dataset as a surrogate and periodically test model on it (though that may not reflect current data drift fully).

They might also ask “Have you implemented automated drift or performance alerts?” – I’d mention using something like EvidentlyAI library integrated with our monitoring to track drift metrics, or custom scripts that run daily and push a metric like “accuracy_last_30days” to Grafana and alert if below threshold.

Finally, on “compliance”: If model performance degrade could hurt SLAs (some companies might have promises), that’s additional reason to be vigilant. Probably not needed to mention unless relevant.

Questions to Ask Jakub (the Interviewer)

Finally, when the interviewer asks if you have any questions, it’s important to ask thoughtful questions that show your genuine interest in the role, the team, and monday.com’s ML infrastructure. Here are some good questions I’d consider asking Jakub Sokół:

“How does monday.com currently handle observability for ML models? I’m curious which tools or dashboards the team uses to monitor model performance and reliability, and what metrics you’ve found most valuable on the Observability team.”
(This shows interest in his expertise in observability and allows him to discuss tools like Prometheus, Datadog, etc., and any challenges specific to monitoring ML.)

“Jakub, given your background in Kubernetes and cloud-native tech, what are some of the advanced Kubernetes practices or tools the team has adopted for the ML infrastructure? For example, do you use things like service mesh, KEDA for event-driven scaling, or custom operators in managing ML workloads?”
(This demonstrates my knowledge and eagerness to learn their specific setup, and lets him highlight cool implementations they have – possibly ties into his certs like CKA/CKS showing I value those skills.)

“Security is a big part of your expertise (noticing your CKS). How does monday.com ensure the ML platform is secure? For instance, do you isolate certain sensitive workloads, or implement any specific Kubernetes security measures for model deployments?”
(This not only flatters his security knowledge but also gets insight into compliance measures Monday takes, aligning with the privacy-first mention. It shows I’m thinking about security as well.)

“Can you describe the collaboration between the Observability team (or DevOps teams) and the data science/AI teams at monday.com? How do you work together when deploying a new model or troubleshooting an issue? I’m interested in the cross-team dynamics.”
(This shows I care about teamwork and understanding processes, plus he can share how their org is structured, e.g., is MLOps part of DevOps or separate, etc.)

“What do you see as the biggest challenge in scaling monday.com’s AI features over the next year or two, and how might this role contribute to solving it?”
(This lets him speak about future plans – maybe more global rollout, more models, latency targets, etc., and positions me as thinking long-term about contributions.)

“Monday.com has a multi-region architecture for compliance. How does that affect the ML infrastructure? For example, do you train separate models per region due to data residency, or how do you deploy models across regions to ensure consistent service?”
(This shows I’m aware of their multi-region design
engineering.monday.com
 and want to learn how it impacts MLOps, a nuanced question likely up his alley.)

“What is the maturity level of the current ML platform? Are you mostly building things from scratch or improving an existing platform? I’d love to know if there’s flexibility to introduce new technologies or if standardization on certain tools has already happened.”
(This question will help me gauge how much is greenfield vs established, and it signals I’m thinking about how I can fit in and contribute tech-wise.)

“Given Monday.com’s culture of building internal tools (like Sphera and Ensemble), is the team considering building any internal tools for MLOps (perhaps an internal feature store or monitoring suite), or will we lean on open-source solutions?”
(This connects to knowledge from their engineering blog about building tools
engineering.monday.com
engineering.monday.com
 and asks where MLOps stands. It also shows I did my research and know their tendencies.)

“On the Observability side, what kinds of incidents or issues have you seen with ML systems in production at Monday? For example, have there been cases of concept drift, or more so typical infra issues like scaling and outages? I’d like to understand what real problems we focus on.”
(Jakub can then share war stories or key learning points; it shows I’m thinking practically about the challenges.)

“What do you personally find exciting about monday.com’s AI initiatives and where do you see the Observability/DevOps team making the biggest impact on those?”
(This gives him a chance to express enthusiasm and gives me insight into the company’s direction, plus builds rapport by showing interest in his perspective.)

Each of these questions is open-ended and invites a detailed answer, which can lead to a meaningful discussion and show that I’m seriously engaged with the technical and strategic aspects of the role.

Final Preparation Checklist

To wrap up, here’s a checklist to ensure I’m fully prepared for the interview and role:

 Review Monday.com’s architecture & AI features: Refresh key points about multi-region, microservices, internal tools (Sphera, Ensemble)
engineering.monday.com
engineering.monday.com
, and AI capabilities (AI assistant, AI Blocks) to weave into answers and show context awareness.

 Brush up on Kubernetes deep-dives: Especially focus on advanced topics relevant to MLOps – e.g., stateful vs stateless sets, GPU scheduling, network policies, Helm/ArgoCD usage, service mesh patterns. Be ready to discuss how I’ve used K8s for ML (maybe practice explaining KServe vs custom deploy).

 Prepare concrete examples: Recall specific incidents (like the one I described) and outcomes to use in behavioral answers (STAR format). Also have in mind examples of optimizations I did, cost savings achieved, etc., to lend credibility.

 Rehearse system design scenarios: Particularly multi-tenant design, canary deployment strategy, and real-time feature pipelines. Practice structuring the answer clearly (as I did in scenarios), so in the interview I can organize my thoughts under pressure.

 Revise key tools and terms: Ensure I can confidently talk about tools I mentioned – e.g., Prometheus, Datadog, Kafka, SageMaker, Terraform – what they do and how I used them. Jakub might drill into any I bring up. Also revision of ML specifics like model quantization, ONNX, etc., since I raised them.

 Plan clarifying questions: In case a question is slightly unclear, be ready to ask a focused clarification (e.g., “Are we focusing on training or inference here?”) – this shows I think before answering and ensures I hit what they want.

 Check logistics: If coding on CoderPad, be ready to write in Python clearly – practice a couple of sample problems (like some we listed) to get in the groove of writing correct, efficient code in a timed scenario.

 Mindset and communication: Be prepared to think aloud in system design (explain assumptions, ask if that aligns with Monday’s case). For behavioral, remember to mention Situation, Task, Action, Result structure.

 Last read of job description and resume alignment: Map how my experience lines up with each responsibility and be prepared to highlight those connections. For example, on “on-call support”
justjoin.it
, have my story ready; on “optimizing GPU usage”
justjoin.it
, cite my experience doing that.

 Prepare environment and notes: Have a few key data (customer count, region info) noted mentally or on scratch paper so I don’t blank – though I won’t explicitly use notes in interview, internalizing them helps. Ensure stable internet, etc., as it’s likely remote.

 Sleep well & stay confident: It’s a high-stakes interview, but I’ve done thorough research and practice. Plan to convey enthusiasm for both ML and DevOps sides, and demonstrate I can bridge those effectively for Monday.com.

By following this checklist and the detailed preparation above, I’ll be in a strong position to impress Jakub and the monday.com team with both my technical depth and my understanding of their specific context. Good luck to me!