MONDAY.COM MLOPS INTERVIEW - CODING CHALLENGES (15-MINUTE PROBLEMS)

These are practical coding exercises you might encounter. Focus on clean, working code with good practices.

================================================================================
CHALLENGE 1: DYNAMIC BATCHING FOR ML INFERENCE
================================================================================

Problem:
Implement a batch collector that groups incoming inference requests to maximize 
GPU throughput. Collect requests up to MAX_BATCH_SIZE or MAX_WAIT_TIME (whichever 
comes first), then process the batch together.

Requirements:
- Async implementation using asyncio
- Return individual results to each request
- Handle errors gracefully

Solution:

```python
import asyncio
from typing import Any, List, Tuple, Callable
from dataclasses import dataclass
import time

@dataclass
class BatchConfig:
    max_batch_size: int = 16
    max_wait_ms: int = 50

class BatchCollector:
    def __init__(self, inference_fn: Callable, config: BatchConfig):
        self.inference_fn = inference_fn
        self.config = config
        self.queue: asyncio.Queue = asyncio.Queue()
        self.worker_task = None
    
    async def start(self):
        """Start the background worker"""
        if self.worker_task is None:
            self.worker_task = asyncio.create_task(self._worker())
    
    async def predict(self, input_data: Any) -> Any:
        """Submit a request and wait for result"""
        future = asyncio.Future()
        await self.queue.put((input_data, future))
        return await future
    
    async def _worker(self):
        """Background worker that batches requests"""
        while True:
            batch = []
            deadline = time.time() + (self.config.max_wait_ms / 1000)
            
            # Collect first request (wait indefinitely)
            try:
                item = await asyncio.wait_for(
                    self.queue.get(), 
                    timeout=self.config.max_wait_ms / 1000
                )
                batch.append(item)
            except asyncio.TimeoutError:
                continue
            
            # Collect additional requests until deadline or batch full
            while len(batch) < self.config.max_batch_size:
                remaining = deadline - time.time()
                if remaining <= 0:
                    break
                
                try:
                    item = await asyncio.wait_for(self.queue.get(), timeout=remaining)
                    batch.append(item)
                except asyncio.TimeoutError:
                    break
            
            if not batch:
                continue
            
            # Process batch
            inputs = [item[0] for item in batch]
            futures = [item[1] for item in batch]
            
            try:
                results = await self.inference_fn(inputs)
                for future, result in zip(futures, results):
                    future.set_result(result)
            except Exception as e:
                for future in futures:
                    future.set_exception(e)

# Example usage:
async def mock_inference(batch: List[str]) -> List[str]:
    await asyncio.sleep(0.01)  # Simulate GPU work
    return [s.upper() for s in batch]

async def main():
    collector = BatchCollector(mock_inference, BatchConfig(max_batch_size=8))
    await collector.start()
    
    # Simulate concurrent requests
    tasks = [collector.predict(f"input_{i}") for i in range(20)]
    results = await asyncio.gather(*tasks)
    print(results)

# Time Complexity: O(n) where n is total requests
# Space Complexity: O(batch_size)
```

================================================================================
CHALLENGE 2: CIRCUIT BREAKER FOR MODEL CALLS
================================================================================

Problem:
Implement a circuit breaker pattern to protect the system when a model service
is failing. After N consecutive failures, open the circuit and reject requests
immediately. After a timeout, allow one test request through (half-open state).

Solution:

```python
import asyncio
import time
from enum import Enum
from typing import Callable, Any

class CircuitState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Blocking requests
    HALF_OPEN = "half_open"  # Testing recovery

class CircuitBreaker:
    def __init__(
        self, 
        failure_threshold: int = 5,
        timeout_seconds: int = 60,
        expected_exception: type = Exception
    ):
        self.failure_threshold = failure_threshold
        self.timeout_seconds = timeout_seconds
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    async def call(self, func: Callable, *args, **kwargs) -> Any:
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result
        except self.expected_exception as e:
            self._on_failure()
            raise
    
    def _on_success(self):
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
    
    def _should_attempt_reset(self) -> bool:
        return (
            self.last_failure_time is not None and
            time.time() - self.last_failure_time >= self.timeout_seconds
        )

# Example usage:
async def unreliable_model_call(will_fail: bool = False):
    await asyncio.sleep(0.1)
    if will_fail:
        raise Exception("Model service unavailable")
    return "success"

async def main():
    cb = CircuitBreaker(failure_threshold=3, timeout_seconds=5)
    
    # Trigger failures
    for i in range(5):
        try:
            await cb.call(unreliable_model_call, will_fail=True)
        except Exception as e:
            print(f"Attempt {i+1}: {e}")
    
    # Circuit should be open now
    try:
        await cb.call(unreliable_model_call, will_fail=False)
    except Exception as e:
        print(f"Circuit open: {e}")
    
    # Wait for timeout
    await asyncio.sleep(6)
    result = await cb.call(unreliable_model_call, will_fail=False)
    print(f"After reset: {result}")

# Time Complexity: O(1)
# Space Complexity: O(1)
```

================================================================================
CHALLENGE 3: CANARY DEPLOYMENT HEALTH CHECKER
================================================================================

Problem:
Implement a health checker that monitors metrics during a canary deployment and
decides whether to proceed or rollback. Check error rate, latency, and custom
business metrics. Return a decision after N minutes of monitoring.

Solution:

```python
import asyncio
from typing import Dict, List
from dataclasses import dataclass
from enum import Enum

class Decision(Enum):
    PROCEED = "proceed"
    ROLLBACK = "rollback"
    CONTINUE_MONITORING = "continue"

@dataclass
class Thresholds:
    max_error_rate: float = 0.05  # 5%
    max_p99_latency_ms: float = 1000
    min_success_rate: float = 0.95

@dataclass
class Metrics:
    error_rate: float
    p99_latency_ms: float
    success_rate: float
    request_count: int

class CanaryHealthChecker:
    def __init__(
        self,
        thresholds: Thresholds,
        check_interval_seconds: int = 60,
        min_requests: int = 100
    ):
        self.thresholds = thresholds
        self.check_interval = check_interval_seconds
        self.min_requests = min_requests
    
    async def evaluate(
        self,
        get_canary_metrics: callable,
        get_stable_metrics: callable,
        duration_minutes: int = 10
    ) -> Decision:
        """Monitor canary vs stable version and make deployment decision"""
        
        checks = duration_minutes * 60 // self.check_interval
        
        for i in range(checks):
            await asyncio.sleep(self.check_interval)
            
            canary_metrics = await get_canary_metrics()
            stable_metrics = await get_stable_metrics()
            
            print(f"Check {i+1}/{checks}:")
            print(f"  Canary: {canary_metrics}")
            print(f"  Stable: {stable_metrics}")
            
            # Need minimum traffic to make decision
            if canary_metrics.request_count < self.min_requests:
                print("  -> Not enough traffic yet")
                continue
            
            # Check absolute thresholds
            if canary_metrics.error_rate > self.thresholds.max_error_rate:
                print(f"  -> ROLLBACK: Error rate too high")
                return Decision.ROLLBACK
            
            if canary_metrics.p99_latency_ms > self.thresholds.max_p99_latency_ms:
                print(f"  -> ROLLBACK: Latency too high")
                return Decision.ROLLBACK
            
            # Compare to stable version (canary shouldn't be significantly worse)
            if canary_metrics.error_rate > stable_metrics.error_rate * 2:
                print(f"  -> ROLLBACK: Error rate 2x worse than stable")
                return Decision.ROLLBACK
            
            if canary_metrics.p99_latency_ms > stable_metrics.p99_latency_ms * 1.5:
                print(f"  -> ROLLBACK: Latency 50% worse than stable")
                return Decision.ROLLBACK
            
            print("  -> Canary looks healthy")
        
        print("All checks passed - PROCEED with rollout")
        return Decision.PROCEED

# Example usage:
async def get_canary_metrics() -> Metrics:
    # Simulate fetching from Prometheus/Datadog
    return Metrics(
        error_rate=0.02,
        p99_latency_ms=450,
        success_rate=0.98,
        request_count=500
    )

async def get_stable_metrics() -> Metrics:
    return Metrics(
        error_rate=0.01,
        p99_latency_ms=400,
        success_rate=0.99,
        request_count=5000
    )

async def main():
    checker = CanaryHealthChecker(
        thresholds=Thresholds(),
        check_interval_seconds=5,  # Faster for demo
        min_requests=100
    )
    
    decision = await checker.evaluate(
        get_canary_metrics,
        get_stable_metrics,
        duration_minutes=1  # Short for demo
    )
    
    print(f"\nFinal Decision: {decision}")

# Time Complexity: O(n) where n is number of checks
# Space Complexity: O(1)
```

================================================================================
CHALLENGE 4: FEATURE CACHE WITH EXPIRATION
================================================================================

Problem:
Implement a thread-safe cache for ML feature vectors that supports:
- Get/Set with TTL (time-to-live)
- LRU eviction when capacity is reached
- Automatic cleanup of expired entries

Solution:

```python
import time
import threading
from typing import Any, Optional
from collections import OrderedDict
from dataclasses import dataclass

@dataclass
class CacheEntry:
    value: Any
    expiry_time: float

class FeatureCache:
    def __init__(self, max_size: int = 1000, default_ttl_seconds: int = 300):
        self.max_size = max_size
        self.default_ttl = default_ttl_seconds
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.lock = threading.RLock()
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                self.misses += 1
                return None
            
            entry = self.cache[key]
            
            # Check if expired
            if time.time() > entry.expiry_time:
                del self.cache[key]
                self.misses += 1
                return None
            
            # Move to end (most recently used)
            self.cache.move_to_end(key)
            self.hits += 1
            return entry.value
    
    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        with self.lock:
            ttl = ttl_seconds if ttl_seconds is not None else self.default_ttl
            expiry_time = time.time() + ttl
            
            # Update existing or add new
            if key in self.cache:
                self.cache[key] = CacheEntry(value, expiry_time)
                self.cache.move_to_end(key)
            else:
                # Evict LRU if at capacity
                if len(self.cache) >= self.max_size:
                    self.cache.popitem(last=False)
                
                self.cache[key] = CacheEntry(value, expiry_time)
    
    def delete(self, key: str):
        with self.lock:
            if key in self.cache:
                del self.cache[key]
    
    def clear_expired(self):
        """Remove all expired entries"""
        with self.lock:
            current_time = time.time()
            expired_keys = [
                k for k, v in self.cache.items() 
                if current_time > v.expiry_time
            ]
            for key in expired_keys:
                del self.cache[key]
    
    def get_stats(self) -> dict:
        with self.lock:
            total = self.hits + self.misses
            hit_rate = self.hits / total if total > 0 else 0
            return {
                "size": len(self.cache),
                "capacity": self.max_size,
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": hit_rate
            }

# Example usage:
def main():
    cache = FeatureCache(max_size=3, default_ttl_seconds=2)
    
    # Add items
    cache.set("user_1", [0.1, 0.2, 0.3])
    cache.set("user_2", [0.4, 0.5, 0.6])
    cache.set("user_3", [0.7, 0.8, 0.9])
    
    print("After adding 3 items:", cache.get_stats())
    
    # Access user_1 (move to end)
    print("user_1:", cache.get("user_1"))
    
    # Add user_4 (should evict user_2, the LRU)
    cache.set("user_4", [1.0, 1.1, 1.2])
    print("user_2 (should be evicted):", cache.get("user_2"))
    
    # Wait for expiration
    time.sleep(2.5)
    cache.clear_expired()
    
    print("After expiration:", cache.get_stats())

# Time Complexity: 
#   get: O(1), set: O(1), clear_expired: O(n)
# Space Complexity: O(max_size)
```

================================================================================
CHALLENGE 5: PROMETHEUS METRICS EXPORTER
================================================================================

Problem:
Implement a metrics collector for an ML service that exposes Prometheus-compatible
metrics including: request count, latency histogram, model prediction distribution,
and GPU utilization.

Solution:

```python
from prometheus_client import Counter, Histogram, Gauge, Enum
from prometheus_client import start_http_server
from typing import Dict
import time
import random

class MLServiceMetrics:
    def __init__(self, service_name: str = "ml_inference"):
        # Request metrics
        self.request_count = Counter(
            f'{service_name}_requests_total',
            'Total number of inference requests',
            ['model_version', 'status']
        )
        
        self.request_latency = Histogram(
            f'{service_name}_request_duration_seconds',
            'Request latency in seconds',
            ['model_version'],
            buckets=(0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0)
        )
        
        # Model-specific metrics
        self.prediction_distribution = Counter(
            f'{service_name}_predictions_total',
            'Count of predictions by class',
            ['model_version', 'predicted_class']
        )
        
        self.model_confidence = Histogram(
            f'{service_name}_confidence_score',
            'Model confidence scores',
            ['model_version'],
            buckets=(0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 1.0)
        )
        
        # Infrastructure metrics
        self.gpu_utilization = Gauge(
            f'{service_name}_gpu_utilization_percent',
            'GPU utilization percentage',
            ['gpu_id']
        )
        
        self.gpu_memory_used = Gauge(
            f'{service_name}_gpu_memory_used_bytes',
            'GPU memory used in bytes',
            ['gpu_id']
        )
        
        # Current state
        self.model_state = Enum(
            f'{service_name}_model_state',
            'Current model state',
            ['model_version'],
            states=['loading', 'ready', 'error']
        )
    
    def record_request(
        self,
        model_version: str,
        latency_seconds: float,
        status: str = "success"
    ):
        """Record a completed inference request"""
        self.request_count.labels(
            model_version=model_version,
            status=status
        ).inc()
        
        self.request_latency.labels(
            model_version=model_version
        ).observe(latency_seconds)
    
    def record_prediction(
        self,
        model_version: str,
        predicted_class: str,
        confidence: float
    ):
        """Record model prediction details"""
        self.prediction_distribution.labels(
            model_version=model_version,
            predicted_class=predicted_class
        ).inc()
        
        self.model_confidence.labels(
            model_version=model_version
        ).observe(confidence)
    
    def update_gpu_metrics(self, gpu_stats: Dict[str, Dict]):
        """Update GPU utilization metrics"""
        for gpu_id, stats in gpu_stats.items():
            self.gpu_utilization.labels(gpu_id=gpu_id).set(
                stats['utilization_percent']
            )
            self.gpu_memory_used.labels(gpu_id=gpu_id).set(
                stats['memory_used_bytes']
            )
    
    def set_model_state(self, model_version: str, state: str):
        """Update model state"""
        self.model_state.labels(model_version=model_version).state(state)

# Example usage:
def simulate_inference_service():
    metrics = MLServiceMetrics("sentiment_classifier")
    
    # Start Prometheus HTTP server on port 8000
    start_http_server(8000)
    print("Metrics available at http://localhost:8000/metrics")
    
    model_version = "v2.3"
    metrics.set_model_state(model_version, "ready")
    
    # Simulate requests
    while True:
        start_time = time.time()
        
        # Simulate inference
        time.sleep(random.uniform(0.01, 0.2))
        
        # Random prediction
        classes = ["positive", "neutral", "negative"]
        predicted_class = random.choice(classes)
        confidence = random.uniform(0.7, 0.99)
        
        # Record metrics
        latency = time.time() - start_time
        metrics.record_request(model_version, latency, "success")
        metrics.record_prediction(model_version, predicted_class, confidence)
        
        # Update GPU stats (simulated)
        gpu_stats = {
            "gpu0": {
                "utilization_percent": random.uniform(60, 95),
                "memory_used_bytes": random.uniform(8e9, 15e9)
            }
        }
        metrics.update_gpu_metrics(gpu_stats)
        
        time.sleep(0.1)

# This would run as part of your inference service
# The metrics endpoint can be scraped by Prometheus
```

================================================================================
KEY TAKEAWAYS FOR CODING CHALLENGES
================================================================================

1. CODE QUALITY MATTERS
   - Write clean, readable code with good variable names
   - Add type hints where helpful
   - Handle errors gracefully
   - Include docstrings for complex functions

2. THINK ABOUT SCALE
   - Discuss time/space complexity
   - Consider concurrent access (threading/asyncio)
   - Think about memory limits
   - Plan for failure cases

3. PRODUCTION-READY
   - Logging for debugging
   - Metrics for monitoring
   - Configuration over hardcoding
   - Graceful degradation

4. MLOPS-SPECIFIC
   - Batching for throughput
   - Caching for latency
   - Metrics for observability
   - Versioning for deployments

5. COMMUNICATION
   - Explain your approach before coding
   - Talk through trade-offs
   - Test with simple examples
   - Ask clarifying questions

Good luck with the coding portion! ðŸŽ¯
